{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb150692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyfsl.methods import PrototypicalNetworks\n",
    "from easyfsl.datasets import CUSTOM\n",
    "from easyfsl.samplers import TaskSampler\n",
    "from easyfsl.utils import compute_prototypes\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8da901",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d921737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module) :\n",
    "    def __init__(self, name='efficientnetv2_rw_m', num_classes=0)  :\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = timm.create_model(name, pretrained=True, num_classes=num_classes)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        \n",
    "        self.use_fc = True\n",
    "        \n",
    "    def forward(self, x) :\n",
    "\n",
    "        if self.use_fc:\n",
    "            x = self.model(x)\n",
    "\n",
    "        else :\n",
    "            x = self.model.forward_features(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.flatten(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def set_use_fc(self, x) :\n",
    "        self.use_fc = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ad74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_model_path = './submission/fsl_224_effi_b0/2E_model.pt'\n",
    "convolutional_network = CNN()\n",
    "convolutional_network.set_use_fc(False)\n",
    "few_shot_classifier = PrototypicalNetworks(convolutional_network).to(device)\n",
    "\n",
    "fsm_checkpoint = torch.load(fs_model_path)\n",
    "few_shot_classifier.load_state_dict(fsm_checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865677c9",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f65d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "fsl = defaultdict(list)\n",
    "\n",
    "path = './data/fsl_img/*'\n",
    "label_list = glob(path)\n",
    "label_list.sort(key=lambda x: int(x.split('\\\\')[-1]))\n",
    "\n",
    "for label in label_list :  \n",
    "    fsl['class_roots'].append(label)\n",
    "    fsl['class_names'].append(label.split('\\\\')[-1])\n",
    "    \n",
    "with open('data/fsl_test_supset.json', 'w') as f:\n",
    "    json.dump(fsl, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076bd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUSTOM_DATASET(Dataset) :\n",
    "    def __init__(self, root_path, n_way_start=None, n_way_end=None, n_shot=None, transform=None) :\n",
    "        self.img_list, self.label_list = self.folder_split(root_path, n_way_start, n_way_end, n_shot)\n",
    "        self.n_way_start = n_way_start\n",
    "        self.n_way_end = n_way_end\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) :\n",
    "        assert len(self.img_list) == len(self.label_list), 'Doesn\\'t match between img length and label length'\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        img = cv2.imread(self.img_list[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        img = self.transform(image=img)['image']\n",
    "        label = torch.tensor(self.label_list[idx], dtype=torch.long ) \n",
    "        \n",
    "        return img, label\n",
    "    \n",
    "    def folder_split(self, root_path, n_way_start, n_way_end, n_shot) :\n",
    "        folders = glob(os.path.join(root_path, \"*\"))\n",
    "        folders.sort(key=lambda x: int(x.split('\\\\')[-1]))\n",
    "        folders = folders[n_way_start : n_way_end]\n",
    "\n",
    "        img_list = []\n",
    "        label_list = []\n",
    "        for idx, path in enumerate(folders) :\n",
    "            img_list.extend(glob(os.path.join(path,\"*\"))[:n_shot])\n",
    "            label_list.extend([idx] * len(glob(os.path.join(path,\"*\"))[:n_shot]))\n",
    "        \n",
    "        return img_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ceffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_path = 'data/fsl_img\n",
    "transform = A.Compose([\n",
    "    # H ,W = 1 : 1.5\n",
    "    A.Resize(250, 375),\n",
    "    A.OneOf([\n",
    "        A.RandomCrop(224,224),\n",
    "        A.CenterCrop(224,224)\n",
    "    ], p=1),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_path = 'data/fsl_test_supset'\n",
    "n_way = 5\n",
    "n_shot = 5\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2bcf03",
   "metadata": {},
   "source": [
    "# Prototypes 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_classifier.eval()\n",
    "prototypes = np.array([])\n",
    "for n_way_start in range(0, 50, n_way) :\n",
    "    n_way_end = n_way_start + n_way\n",
    "    \n",
    "    custom_dataset = CUSTOM_DATASET(support_path, n_way_start, n_way_end, n_shot, transform)\n",
    "    dataloader = DataLoader(custom_dataset, batch_size=(n_way_end-n_way_start) * n_shot)\n",
    "    sup_img, sup_label = next(iter(dataloader))\n",
    "    with torch.no_grad() :\n",
    "        sup_feature = few_shot_classifier.backbone.forward(sup_img.to(device))\n",
    "        if n_way_start == 0 :\n",
    "            prototypes = compute_prototypes(sup_feature, sup_label).cpu().detach().numpy()\n",
    "        else : \n",
    "            prototypes = np.concatenate([prototypes, compute_prototypes(sup_feature, sup_label).cpu().detach().numpy()])\n",
    "#     np.save(f'support_feautres/{n_way_start}_{n_way_end}', prototypes.cpu().detach().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00015475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
