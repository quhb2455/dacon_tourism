{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300c4cdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adv_inception_v3',\n",
       " 'bat_resnext26ts',\n",
       " 'beit_base_patch16_224',\n",
       " 'beit_base_patch16_224_in22k',\n",
       " 'beit_base_patch16_384',\n",
       " 'beit_large_patch16_224',\n",
       " 'beit_large_patch16_224_in22k',\n",
       " 'beit_large_patch16_384',\n",
       " 'beit_large_patch16_512',\n",
       " 'botnet26t_256',\n",
       " 'cait_m36_384',\n",
       " 'cait_m48_448',\n",
       " 'cait_s24_224',\n",
       " 'cait_s24_384',\n",
       " 'cait_s36_384',\n",
       " 'cait_xs24_384',\n",
       " 'cait_xxs24_224',\n",
       " 'cait_xxs24_384',\n",
       " 'cait_xxs36_224',\n",
       " 'cait_xxs36_384',\n",
       " 'coat_lite_mini',\n",
       " 'coat_lite_small',\n",
       " 'coat_lite_tiny',\n",
       " 'coat_mini',\n",
       " 'coat_tiny',\n",
       " 'convit_base',\n",
       " 'convit_small',\n",
       " 'convit_tiny',\n",
       " 'convmixer_768_32',\n",
       " 'convmixer_1024_20_ks9_p14',\n",
       " 'convmixer_1536_20',\n",
       " 'convnext_base',\n",
       " 'convnext_base_384_in22ft1k',\n",
       " 'convnext_base_in22ft1k',\n",
       " 'convnext_base_in22k',\n",
       " 'convnext_large',\n",
       " 'convnext_large_384_in22ft1k',\n",
       " 'convnext_large_in22ft1k',\n",
       " 'convnext_large_in22k',\n",
       " 'convnext_nano',\n",
       " 'convnext_small',\n",
       " 'convnext_small_384_in22ft1k',\n",
       " 'convnext_small_in22ft1k',\n",
       " 'convnext_small_in22k',\n",
       " 'convnext_tiny',\n",
       " 'convnext_tiny_384_in22ft1k',\n",
       " 'convnext_tiny_hnf',\n",
       " 'convnext_tiny_in22ft1k',\n",
       " 'convnext_tiny_in22k',\n",
       " 'convnext_xlarge_384_in22ft1k',\n",
       " 'convnext_xlarge_in22ft1k',\n",
       " 'convnext_xlarge_in22k',\n",
       " 'crossvit_9_240',\n",
       " 'crossvit_9_dagger_240',\n",
       " 'crossvit_15_240',\n",
       " 'crossvit_15_dagger_240',\n",
       " 'crossvit_15_dagger_408',\n",
       " 'crossvit_18_240',\n",
       " 'crossvit_18_dagger_240',\n",
       " 'crossvit_18_dagger_408',\n",
       " 'crossvit_base_240',\n",
       " 'crossvit_small_240',\n",
       " 'crossvit_tiny_240',\n",
       " 'cs3darknet_focus_l',\n",
       " 'cs3darknet_focus_m',\n",
       " 'cs3darknet_l',\n",
       " 'cs3darknet_m',\n",
       " 'cs3darknet_x',\n",
       " 'cs3edgenet_x',\n",
       " 'cs3se_edgenet_x',\n",
       " 'cs3sedarknet_l',\n",
       " 'cs3sedarknet_x',\n",
       " 'cspdarknet53',\n",
       " 'cspresnet50',\n",
       " 'cspresnext50',\n",
       " 'darknet53',\n",
       " 'darknetaa53',\n",
       " 'deit3_base_patch16_224',\n",
       " 'deit3_base_patch16_224_in21ft1k',\n",
       " 'deit3_base_patch16_384',\n",
       " 'deit3_base_patch16_384_in21ft1k',\n",
       " 'deit3_huge_patch14_224',\n",
       " 'deit3_huge_patch14_224_in21ft1k',\n",
       " 'deit3_large_patch16_224',\n",
       " 'deit3_large_patch16_224_in21ft1k',\n",
       " 'deit3_large_patch16_384',\n",
       " 'deit3_large_patch16_384_in21ft1k',\n",
       " 'deit3_small_patch16_224',\n",
       " 'deit3_small_patch16_224_in21ft1k',\n",
       " 'deit3_small_patch16_384',\n",
       " 'deit3_small_patch16_384_in21ft1k',\n",
       " 'deit_base_distilled_patch16_224',\n",
       " 'deit_base_distilled_patch16_384',\n",
       " 'deit_base_patch16_224',\n",
       " 'deit_base_patch16_384',\n",
       " 'deit_small_distilled_patch16_224',\n",
       " 'deit_small_patch16_224',\n",
       " 'deit_tiny_distilled_patch16_224',\n",
       " 'deit_tiny_patch16_224',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'densenetblur121d',\n",
       " 'dla34',\n",
       " 'dla46_c',\n",
       " 'dla46x_c',\n",
       " 'dla60',\n",
       " 'dla60_res2net',\n",
       " 'dla60_res2next',\n",
       " 'dla60x',\n",
       " 'dla60x_c',\n",
       " 'dla102',\n",
       " 'dla102x',\n",
       " 'dla102x2',\n",
       " 'dla169',\n",
       " 'dm_nfnet_f0',\n",
       " 'dm_nfnet_f1',\n",
       " 'dm_nfnet_f2',\n",
       " 'dm_nfnet_f3',\n",
       " 'dm_nfnet_f4',\n",
       " 'dm_nfnet_f5',\n",
       " 'dm_nfnet_f6',\n",
       " 'dpn68',\n",
       " 'dpn68b',\n",
       " 'dpn92',\n",
       " 'dpn98',\n",
       " 'dpn107',\n",
       " 'dpn131',\n",
       " 'eca_botnext26ts_256',\n",
       " 'eca_halonext26ts',\n",
       " 'eca_nfnet_l0',\n",
       " 'eca_nfnet_l1',\n",
       " 'eca_nfnet_l2',\n",
       " 'eca_resnet33ts',\n",
       " 'eca_resnext26ts',\n",
       " 'ecaresnet26t',\n",
       " 'ecaresnet50d',\n",
       " 'ecaresnet50d_pruned',\n",
       " 'ecaresnet50t',\n",
       " 'ecaresnet101d',\n",
       " 'ecaresnet101d_pruned',\n",
       " 'ecaresnet269d',\n",
       " 'ecaresnetlight',\n",
       " 'edgenext_small',\n",
       " 'edgenext_small_rw',\n",
       " 'edgenext_x_small',\n",
       " 'edgenext_xx_small',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b1_pruned',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b2_pruned',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b3_pruned',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_el',\n",
       " 'efficientnet_el_pruned',\n",
       " 'efficientnet_em',\n",
       " 'efficientnet_es',\n",
       " 'efficientnet_es_pruned',\n",
       " 'efficientnet_lite0',\n",
       " 'efficientnetv2_rw_m',\n",
       " 'efficientnetv2_rw_s',\n",
       " 'efficientnetv2_rw_t',\n",
       " 'ens_adv_inception_resnet_v2',\n",
       " 'ese_vovnet19b_dw',\n",
       " 'ese_vovnet39b',\n",
       " 'fbnetc_100',\n",
       " 'fbnetv3_b',\n",
       " 'fbnetv3_d',\n",
       " 'fbnetv3_g',\n",
       " 'gc_efficientnetv2_rw_t',\n",
       " 'gcresnet33ts',\n",
       " 'gcresnet50t',\n",
       " 'gcresnext26ts',\n",
       " 'gcresnext50ts',\n",
       " 'gernet_l',\n",
       " 'gernet_m',\n",
       " 'gernet_s',\n",
       " 'ghostnet_100',\n",
       " 'gluon_inception_v3',\n",
       " 'gluon_resnet18_v1b',\n",
       " 'gluon_resnet34_v1b',\n",
       " 'gluon_resnet50_v1b',\n",
       " 'gluon_resnet50_v1c',\n",
       " 'gluon_resnet50_v1d',\n",
       " 'gluon_resnet50_v1s',\n",
       " 'gluon_resnet101_v1b',\n",
       " 'gluon_resnet101_v1c',\n",
       " 'gluon_resnet101_v1d',\n",
       " 'gluon_resnet101_v1s',\n",
       " 'gluon_resnet152_v1b',\n",
       " 'gluon_resnet152_v1c',\n",
       " 'gluon_resnet152_v1d',\n",
       " 'gluon_resnet152_v1s',\n",
       " 'gluon_resnext50_32x4d',\n",
       " 'gluon_resnext101_32x4d',\n",
       " 'gluon_resnext101_64x4d',\n",
       " 'gluon_senet154',\n",
       " 'gluon_seresnext50_32x4d',\n",
       " 'gluon_seresnext101_32x4d',\n",
       " 'gluon_seresnext101_64x4d',\n",
       " 'gluon_xception65',\n",
       " 'gmixer_24_224',\n",
       " 'gmlp_s16_224',\n",
       " 'halo2botnet50ts_256',\n",
       " 'halonet26t',\n",
       " 'halonet50ts',\n",
       " 'haloregnetz_b',\n",
       " 'hardcorenas_a',\n",
       " 'hardcorenas_b',\n",
       " 'hardcorenas_c',\n",
       " 'hardcorenas_d',\n",
       " 'hardcorenas_e',\n",
       " 'hardcorenas_f',\n",
       " 'hrnet_w18',\n",
       " 'hrnet_w18_small',\n",
       " 'hrnet_w18_small_v2',\n",
       " 'hrnet_w30',\n",
       " 'hrnet_w32',\n",
       " 'hrnet_w40',\n",
       " 'hrnet_w44',\n",
       " 'hrnet_w48',\n",
       " 'hrnet_w64',\n",
       " 'ig_resnext101_32x8d',\n",
       " 'ig_resnext101_32x16d',\n",
       " 'ig_resnext101_32x32d',\n",
       " 'ig_resnext101_32x48d',\n",
       " 'inception_resnet_v2',\n",
       " 'inception_v3',\n",
       " 'inception_v4',\n",
       " 'jx_nest_base',\n",
       " 'jx_nest_small',\n",
       " 'jx_nest_tiny',\n",
       " 'lambda_resnet26rpt_256',\n",
       " 'lambda_resnet26t',\n",
       " 'lambda_resnet50ts',\n",
       " 'lamhalobotnet50ts_256',\n",
       " 'lcnet_050',\n",
       " 'lcnet_075',\n",
       " 'lcnet_100',\n",
       " 'legacy_senet154',\n",
       " 'legacy_seresnet18',\n",
       " 'legacy_seresnet34',\n",
       " 'legacy_seresnet50',\n",
       " 'legacy_seresnet101',\n",
       " 'legacy_seresnet152',\n",
       " 'legacy_seresnext26_32x4d',\n",
       " 'legacy_seresnext50_32x4d',\n",
       " 'legacy_seresnext101_32x4d',\n",
       " 'levit_128',\n",
       " 'levit_128s',\n",
       " 'levit_192',\n",
       " 'levit_256',\n",
       " 'levit_384',\n",
       " 'mixer_b16_224',\n",
       " 'mixer_b16_224_in21k',\n",
       " 'mixer_b16_224_miil',\n",
       " 'mixer_b16_224_miil_in21k',\n",
       " 'mixer_l16_224',\n",
       " 'mixer_l16_224_in21k',\n",
       " 'mixnet_l',\n",
       " 'mixnet_m',\n",
       " 'mixnet_s',\n",
       " 'mixnet_xl',\n",
       " 'mnasnet_100',\n",
       " 'mnasnet_small',\n",
       " 'mobilenetv2_050',\n",
       " 'mobilenetv2_100',\n",
       " 'mobilenetv2_110d',\n",
       " 'mobilenetv2_120d',\n",
       " 'mobilenetv2_140',\n",
       " 'mobilenetv3_large_100',\n",
       " 'mobilenetv3_large_100_miil',\n",
       " 'mobilenetv3_large_100_miil_in21k',\n",
       " 'mobilenetv3_rw',\n",
       " 'mobilenetv3_small_050',\n",
       " 'mobilenetv3_small_075',\n",
       " 'mobilenetv3_small_100',\n",
       " 'mobilevit_s',\n",
       " 'mobilevit_xs',\n",
       " 'mobilevit_xxs',\n",
       " 'mobilevitv2_050',\n",
       " 'mobilevitv2_075',\n",
       " 'mobilevitv2_100',\n",
       " 'mobilevitv2_125',\n",
       " 'mobilevitv2_150',\n",
       " 'mobilevitv2_150_384_in22ft1k',\n",
       " 'mobilevitv2_150_in22ft1k',\n",
       " 'mobilevitv2_175',\n",
       " 'mobilevitv2_175_384_in22ft1k',\n",
       " 'mobilevitv2_175_in22ft1k',\n",
       " 'mobilevitv2_200',\n",
       " 'mobilevitv2_200_384_in22ft1k',\n",
       " 'mobilevitv2_200_in22ft1k',\n",
       " 'nasnetalarge',\n",
       " 'nf_regnet_b1',\n",
       " 'nf_resnet50',\n",
       " 'nfnet_l0',\n",
       " 'pit_b_224',\n",
       " 'pit_b_distilled_224',\n",
       " 'pit_s_224',\n",
       " 'pit_s_distilled_224',\n",
       " 'pit_ti_224',\n",
       " 'pit_ti_distilled_224',\n",
       " 'pit_xs_224',\n",
       " 'pit_xs_distilled_224',\n",
       " 'pnasnet5large',\n",
       " 'poolformer_m36',\n",
       " 'poolformer_m48',\n",
       " 'poolformer_s12',\n",
       " 'poolformer_s24',\n",
       " 'poolformer_s36',\n",
       " 'regnetv_040',\n",
       " 'regnetv_064',\n",
       " 'regnetx_002',\n",
       " 'regnetx_004',\n",
       " 'regnetx_006',\n",
       " 'regnetx_008',\n",
       " 'regnetx_016',\n",
       " 'regnetx_032',\n",
       " 'regnetx_040',\n",
       " 'regnetx_064',\n",
       " 'regnetx_080',\n",
       " 'regnetx_120',\n",
       " 'regnetx_160',\n",
       " 'regnetx_320',\n",
       " 'regnety_002',\n",
       " 'regnety_004',\n",
       " 'regnety_006',\n",
       " 'regnety_008',\n",
       " 'regnety_016',\n",
       " 'regnety_032',\n",
       " 'regnety_040',\n",
       " 'regnety_064',\n",
       " 'regnety_080',\n",
       " 'regnety_120',\n",
       " 'regnety_160',\n",
       " 'regnety_320',\n",
       " 'regnetz_040',\n",
       " 'regnetz_040h',\n",
       " 'regnetz_b16',\n",
       " 'regnetz_c16',\n",
       " 'regnetz_c16_evos',\n",
       " 'regnetz_d8',\n",
       " 'regnetz_d8_evos',\n",
       " 'regnetz_d32',\n",
       " 'regnetz_e8',\n",
       " 'repvgg_a2',\n",
       " 'repvgg_b0',\n",
       " 'repvgg_b1',\n",
       " 'repvgg_b1g4',\n",
       " 'repvgg_b2',\n",
       " 'repvgg_b2g4',\n",
       " 'repvgg_b3',\n",
       " 'repvgg_b3g4',\n",
       " 'res2net50_14w_8s',\n",
       " 'res2net50_26w_4s',\n",
       " 'res2net50_26w_6s',\n",
       " 'res2net50_26w_8s',\n",
       " 'res2net50_48w_2s',\n",
       " 'res2net101_26w_4s',\n",
       " 'res2next50',\n",
       " 'resmlp_12_224',\n",
       " 'resmlp_12_224_dino',\n",
       " 'resmlp_12_distilled_224',\n",
       " 'resmlp_24_224',\n",
       " 'resmlp_24_224_dino',\n",
       " 'resmlp_24_distilled_224',\n",
       " 'resmlp_36_224',\n",
       " 'resmlp_36_distilled_224',\n",
       " 'resmlp_big_24_224',\n",
       " 'resmlp_big_24_224_in22ft1k',\n",
       " 'resmlp_big_24_distilled_224',\n",
       " 'resnest14d',\n",
       " 'resnest26d',\n",
       " 'resnest50d',\n",
       " 'resnest50d_1s4x24d',\n",
       " 'resnest50d_4s2x40d',\n",
       " 'resnest101e',\n",
       " 'resnest200e',\n",
       " 'resnest269e',\n",
       " 'resnet10t',\n",
       " 'resnet14t',\n",
       " 'resnet18',\n",
       " 'resnet18d',\n",
       " 'resnet26',\n",
       " 'resnet26d',\n",
       " 'resnet26t',\n",
       " 'resnet32ts',\n",
       " 'resnet33ts',\n",
       " 'resnet34',\n",
       " 'resnet34d',\n",
       " 'resnet50',\n",
       " 'resnet50_gn',\n",
       " 'resnet50d',\n",
       " 'resnet51q',\n",
       " 'resnet61q',\n",
       " 'resnet101',\n",
       " 'resnet101d',\n",
       " 'resnet152',\n",
       " 'resnet152d',\n",
       " 'resnet200d',\n",
       " 'resnetaa50',\n",
       " 'resnetblur50',\n",
       " 'resnetrs50',\n",
       " 'resnetrs101',\n",
       " 'resnetrs152',\n",
       " 'resnetrs200',\n",
       " 'resnetrs270',\n",
       " 'resnetrs350',\n",
       " 'resnetrs420',\n",
       " 'resnetv2_50',\n",
       " 'resnetv2_50d_evos',\n",
       " 'resnetv2_50d_gn',\n",
       " 'resnetv2_50x1_bit_distilled',\n",
       " 'resnetv2_50x1_bitm',\n",
       " 'resnetv2_50x1_bitm_in21k',\n",
       " 'resnetv2_50x3_bitm',\n",
       " 'resnetv2_50x3_bitm_in21k',\n",
       " 'resnetv2_101',\n",
       " 'resnetv2_101x1_bitm',\n",
       " 'resnetv2_101x1_bitm_in21k',\n",
       " 'resnetv2_101x3_bitm',\n",
       " 'resnetv2_101x3_bitm_in21k',\n",
       " 'resnetv2_152x2_bit_teacher',\n",
       " 'resnetv2_152x2_bit_teacher_384',\n",
       " 'resnetv2_152x2_bitm',\n",
       " 'resnetv2_152x2_bitm_in21k',\n",
       " 'resnetv2_152x4_bitm',\n",
       " 'resnetv2_152x4_bitm_in21k',\n",
       " 'resnext26ts',\n",
       " 'resnext50_32x4d',\n",
       " 'resnext50d_32x4d',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'rexnet_100',\n",
       " 'rexnet_130',\n",
       " 'rexnet_150',\n",
       " 'rexnet_200',\n",
       " 'sebotnet33ts_256',\n",
       " 'sehalonet33ts',\n",
       " 'selecsls42b',\n",
       " 'selecsls60',\n",
       " 'selecsls60b',\n",
       " 'semnasnet_075',\n",
       " 'semnasnet_100',\n",
       " 'sequencer2d_l',\n",
       " 'sequencer2d_m',\n",
       " 'sequencer2d_s',\n",
       " 'seresnet33ts',\n",
       " 'seresnet50',\n",
       " 'seresnet152d',\n",
       " 'seresnext26d_32x4d',\n",
       " 'seresnext26t_32x4d',\n",
       " 'seresnext26ts',\n",
       " 'seresnext50_32x4d',\n",
       " 'seresnext101_32x8d',\n",
       " 'seresnext101d_32x8d',\n",
       " 'seresnextaa101d_32x8d',\n",
       " 'skresnet18',\n",
       " 'skresnet34',\n",
       " 'skresnext50_32x4d',\n",
       " 'spnasnet_100',\n",
       " 'ssl_resnet18',\n",
       " 'ssl_resnet50',\n",
       " 'ssl_resnext50_32x4d',\n",
       " 'ssl_resnext101_32x4d',\n",
       " 'ssl_resnext101_32x8d',\n",
       " 'ssl_resnext101_32x16d',\n",
       " 'swin_base_patch4_window7_224',\n",
       " 'swin_base_patch4_window7_224_in22k',\n",
       " 'swin_base_patch4_window12_384',\n",
       " 'swin_base_patch4_window12_384_in22k',\n",
       " 'swin_large_patch4_window7_224',\n",
       " 'swin_large_patch4_window7_224_in22k',\n",
       " 'swin_large_patch4_window12_384',\n",
       " 'swin_large_patch4_window12_384_in22k',\n",
       " 'swin_s3_base_224',\n",
       " 'swin_s3_small_224',\n",
       " 'swin_s3_tiny_224',\n",
       " 'swin_small_patch4_window7_224',\n",
       " 'swin_tiny_patch4_window7_224',\n",
       " 'swinv2_base_window8_256',\n",
       " 'swinv2_base_window12_192_22k',\n",
       " 'swinv2_base_window12to16_192to256_22kft1k',\n",
       " 'swinv2_base_window12to24_192to384_22kft1k',\n",
       " 'swinv2_base_window16_256',\n",
       " 'swinv2_cr_small_224',\n",
       " 'swinv2_cr_small_ns_224',\n",
       " 'swinv2_cr_tiny_ns_224',\n",
       " 'swinv2_large_window12_192_22k',\n",
       " 'swinv2_large_window12to16_192to256_22kft1k',\n",
       " 'swinv2_large_window12to24_192to384_22kft1k',\n",
       " 'swinv2_small_window8_256',\n",
       " 'swinv2_small_window16_256',\n",
       " 'swinv2_tiny_window8_256',\n",
       " 'swinv2_tiny_window16_256',\n",
       " 'swsl_resnet18',\n",
       " 'swsl_resnet50',\n",
       " 'swsl_resnext50_32x4d',\n",
       " 'swsl_resnext101_32x4d',\n",
       " 'swsl_resnext101_32x8d',\n",
       " 'swsl_resnext101_32x16d',\n",
       " 'tf_efficientnet_b0',\n",
       " 'tf_efficientnet_b0_ap',\n",
       " 'tf_efficientnet_b0_ns',\n",
       " 'tf_efficientnet_b1',\n",
       " 'tf_efficientnet_b1_ap',\n",
       " 'tf_efficientnet_b1_ns',\n",
       " 'tf_efficientnet_b2',\n",
       " 'tf_efficientnet_b2_ap',\n",
       " 'tf_efficientnet_b2_ns',\n",
       " 'tf_efficientnet_b3',\n",
       " 'tf_efficientnet_b3_ap',\n",
       " 'tf_efficientnet_b3_ns',\n",
       " 'tf_efficientnet_b4',\n",
       " 'tf_efficientnet_b4_ap',\n",
       " 'tf_efficientnet_b4_ns',\n",
       " 'tf_efficientnet_b5',\n",
       " 'tf_efficientnet_b5_ap',\n",
       " 'tf_efficientnet_b5_ns',\n",
       " 'tf_efficientnet_b6',\n",
       " 'tf_efficientnet_b6_ap',\n",
       " 'tf_efficientnet_b6_ns',\n",
       " 'tf_efficientnet_b7',\n",
       " 'tf_efficientnet_b7_ap',\n",
       " 'tf_efficientnet_b7_ns',\n",
       " 'tf_efficientnet_b8',\n",
       " 'tf_efficientnet_b8_ap',\n",
       " 'tf_efficientnet_cc_b0_4e',\n",
       " 'tf_efficientnet_cc_b0_8e',\n",
       " 'tf_efficientnet_cc_b1_8e',\n",
       " 'tf_efficientnet_el',\n",
       " 'tf_efficientnet_em',\n",
       " 'tf_efficientnet_es',\n",
       " 'tf_efficientnet_l2_ns',\n",
       " 'tf_efficientnet_l2_ns_475',\n",
       " 'tf_efficientnet_lite0',\n",
       " 'tf_efficientnet_lite1',\n",
       " 'tf_efficientnet_lite2',\n",
       " 'tf_efficientnet_lite3',\n",
       " 'tf_efficientnet_lite4',\n",
       " 'tf_efficientnetv2_b0',\n",
       " 'tf_efficientnetv2_b1',\n",
       " 'tf_efficientnetv2_b2',\n",
       " 'tf_efficientnetv2_b3',\n",
       " 'tf_efficientnetv2_l',\n",
       " 'tf_efficientnetv2_l_in21ft1k',\n",
       " 'tf_efficientnetv2_l_in21k',\n",
       " 'tf_efficientnetv2_m',\n",
       " 'tf_efficientnetv2_m_in21ft1k',\n",
       " 'tf_efficientnetv2_m_in21k',\n",
       " 'tf_efficientnetv2_s',\n",
       " 'tf_efficientnetv2_s_in21ft1k',\n",
       " 'tf_efficientnetv2_s_in21k',\n",
       " 'tf_efficientnetv2_xl_in21ft1k',\n",
       " 'tf_efficientnetv2_xl_in21k',\n",
       " 'tf_inception_v3',\n",
       " 'tf_mixnet_l',\n",
       " 'tf_mixnet_m',\n",
       " 'tf_mixnet_s',\n",
       " 'tf_mobilenetv3_large_075',\n",
       " 'tf_mobilenetv3_large_100',\n",
       " 'tf_mobilenetv3_large_minimal_100',\n",
       " 'tf_mobilenetv3_small_075',\n",
       " 'tf_mobilenetv3_small_100',\n",
       " 'tf_mobilenetv3_small_minimal_100',\n",
       " 'tinynet_a',\n",
       " 'tinynet_b',\n",
       " 'tinynet_c',\n",
       " 'tinynet_d',\n",
       " 'tinynet_e',\n",
       " 'tnt_s_patch16_224',\n",
       " 'tresnet_l',\n",
       " 'tresnet_l_448',\n",
       " 'tresnet_m',\n",
       " 'tresnet_m_448',\n",
       " 'tresnet_m_miil_in21k',\n",
       " 'tresnet_xl',\n",
       " 'tresnet_xl_448',\n",
       " 'tv_densenet121',\n",
       " 'tv_resnet34',\n",
       " 'tv_resnet50',\n",
       " 'tv_resnet101',\n",
       " 'tv_resnet152',\n",
       " 'tv_resnext50_32x4d',\n",
       " 'twins_pcpvt_base',\n",
       " 'twins_pcpvt_large',\n",
       " 'twins_pcpvt_small',\n",
       " 'twins_svt_base',\n",
       " 'twins_svt_large',\n",
       " 'twins_svt_small',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'visformer_small',\n",
       " 'vit_base_patch8_224',\n",
       " 'vit_base_patch8_224_dino',\n",
       " 'vit_base_patch8_224_in21k',\n",
       " 'vit_base_patch16_224',\n",
       " 'vit_base_patch16_224_dino',\n",
       " 'vit_base_patch16_224_in21k',\n",
       " 'vit_base_patch16_224_miil',\n",
       " 'vit_base_patch16_224_miil_in21k',\n",
       " 'vit_base_patch16_224_sam',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_base_patch16_rpn_224',\n",
       " 'vit_base_patch32_224',\n",
       " 'vit_base_patch32_224_in21k',\n",
       " 'vit_base_patch32_224_sam',\n",
       " 'vit_base_patch32_384',\n",
       " 'vit_base_r50_s16_224_in21k',\n",
       " 'vit_base_r50_s16_384',\n",
       " 'vit_huge_patch14_224_in21k',\n",
       " 'vit_large_patch16_224',\n",
       " 'vit_large_patch16_224_in21k',\n",
       " 'vit_large_patch16_384',\n",
       " 'vit_large_patch32_224_in21k',\n",
       " 'vit_large_patch32_384',\n",
       " 'vit_large_r50_s32_224',\n",
       " 'vit_large_r50_s32_224_in21k',\n",
       " 'vit_large_r50_s32_384',\n",
       " 'vit_relpos_base_patch16_224',\n",
       " 'vit_relpos_base_patch16_clsgap_224',\n",
       " 'vit_relpos_base_patch32_plus_rpn_256',\n",
       " 'vit_relpos_medium_patch16_224',\n",
       " 'vit_relpos_medium_patch16_cls_224',\n",
       " 'vit_relpos_medium_patch16_rpn_224',\n",
       " 'vit_relpos_small_patch16_224',\n",
       " 'vit_small_patch8_224_dino',\n",
       " 'vit_small_patch16_224',\n",
       " 'vit_small_patch16_224_dino',\n",
       " 'vit_small_patch16_224_in21k',\n",
       " 'vit_small_patch16_384',\n",
       " 'vit_small_patch32_224',\n",
       " 'vit_small_patch32_224_in21k',\n",
       " 'vit_small_patch32_384',\n",
       " 'vit_small_r26_s32_224',\n",
       " 'vit_small_r26_s32_224_in21k',\n",
       " 'vit_small_r26_s32_384',\n",
       " 'vit_srelpos_medium_patch16_224',\n",
       " 'vit_srelpos_small_patch16_224',\n",
       " 'vit_tiny_patch16_224',\n",
       " 'vit_tiny_patch16_224_in21k',\n",
       " 'vit_tiny_patch16_384',\n",
       " 'vit_tiny_r_s16_p8_224',\n",
       " 'vit_tiny_r_s16_p8_224_in21k',\n",
       " 'vit_tiny_r_s16_p8_384',\n",
       " 'volo_d1_224',\n",
       " 'volo_d1_384',\n",
       " 'volo_d2_224',\n",
       " 'volo_d2_384',\n",
       " 'volo_d3_224',\n",
       " 'volo_d3_448',\n",
       " 'volo_d4_224',\n",
       " 'volo_d4_448',\n",
       " 'volo_d5_224',\n",
       " 'volo_d5_448',\n",
       " 'volo_d5_512',\n",
       " 'wide_resnet50_2',\n",
       " 'wide_resnet101_2',\n",
       " 'xception',\n",
       " 'xception41',\n",
       " 'xception41p',\n",
       " 'xception65',\n",
       " 'xception65p',\n",
       " 'xception71',\n",
       " 'xcit_large_24_p8_224',\n",
       " 'xcit_large_24_p8_224_dist',\n",
       " 'xcit_large_24_p8_384_dist',\n",
       " 'xcit_large_24_p16_224',\n",
       " 'xcit_large_24_p16_224_dist',\n",
       " 'xcit_large_24_p16_384_dist',\n",
       " 'xcit_medium_24_p8_224',\n",
       " 'xcit_medium_24_p8_224_dist',\n",
       " 'xcit_medium_24_p8_384_dist',\n",
       " 'xcit_medium_24_p16_224',\n",
       " 'xcit_medium_24_p16_224_dist',\n",
       " 'xcit_medium_24_p16_384_dist',\n",
       " 'xcit_nano_12_p8_224',\n",
       " 'xcit_nano_12_p8_224_dist',\n",
       " 'xcit_nano_12_p8_384_dist',\n",
       " 'xcit_nano_12_p16_224',\n",
       " 'xcit_nano_12_p16_224_dist',\n",
       " 'xcit_nano_12_p16_384_dist',\n",
       " 'xcit_small_12_p8_224',\n",
       " 'xcit_small_12_p8_224_dist',\n",
       " 'xcit_small_12_p8_384_dist',\n",
       " 'xcit_small_12_p16_224',\n",
       " 'xcit_small_12_p16_224_dist',\n",
       " 'xcit_small_12_p16_384_dist',\n",
       " 'xcit_small_24_p8_224',\n",
       " 'xcit_small_24_p8_224_dist',\n",
       " 'xcit_small_24_p8_384_dist',\n",
       " 'xcit_small_24_p16_224',\n",
       " 'xcit_small_24_p16_224_dist',\n",
       " 'xcit_small_24_p16_384_dist',\n",
       " 'xcit_tiny_12_p8_224',\n",
       " 'xcit_tiny_12_p8_224_dist',\n",
       " 'xcit_tiny_12_p8_384_dist',\n",
       " 'xcit_tiny_12_p16_224',\n",
       " 'xcit_tiny_12_p16_224_dist',\n",
       " 'xcit_tiny_12_p16_384_dist',\n",
       " 'xcit_tiny_24_p8_224',\n",
       " 'xcit_tiny_24_p8_224_dist',\n",
       " 'xcit_tiny_24_p8_384_dist',\n",
       " 'xcit_tiny_24_p16_224',\n",
       " 'xcit_tiny_24_p16_224_dist',\n",
       " 'xcit_tiny_24_p16_384_dist']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4585feee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 577, 768])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'LayerNorm1d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m o \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward_features(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m384\u001b[39m, \u001b[38;5;241m384\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(o\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 9\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm1d\u001b[49m(\u001b[38;5;241m1\u001b[39m)(o)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(o\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     12\u001b[0m o \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool2d(\u001b[38;5;241m1\u001b[39m)(o)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'LayerNorm1d'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "model_name = 'beit_base_patch16_384'\n",
    "m = timm.create_model(model_name, pretrained=True, num_classes=0, global_pool='')\n",
    "m.to(\"cuda\").eval()\n",
    "o = m.forward_features(torch.randn(2, 3, 384, 384).to(\"cuda\"))\n",
    "print(o.shape)\n",
    "\n",
    "o = nn.LayerNorm1d(1)(o)\n",
    "print(o.shape)\n",
    "\n",
    "o = nn.AdaptiveAvgPool2d(1)(o)\n",
    "print(o.shape)\n",
    "\n",
    "o = nn.Flatten()(o)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5f7fc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Beit(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'beit_base_patch16_384'\n",
    "m = timm.create_model(model_name, pretrained=True, num_classes=10)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1cffff1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 15,  0, 82, 23])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.tensor([0,15,0,82,23])\n",
    "display(z)\n",
    "a = torch.nonzero(z)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43b07542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.22371325]),\n",
       " array([0.95212619]),\n",
       " array([0.39487315]),\n",
       " array([0.83614626]),\n",
       " array([0.83483668]),\n",
       " array([0.45732284]),\n",
       " array([0.83627024]),\n",
       " array([0.80616413]),\n",
       " array([0.25990345]),\n",
       " array([0.39417669])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "z = []\n",
    "for _ in range(10) :\n",
    "    cx = np.random.rand(1)\n",
    "    z.append(cx)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75eeb837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quhb2\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\torch\\functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpooled shape: torch.Size([2, 577, 768])\n",
      "Unpooled shape: torch.Size([2, 768, 1])\n",
      "Unpooled shape: torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "# efficientnet_b0\n",
    "m = timm.create_model('beit_base_patch16_384', pretrained=True, num_classes=0, global_pool='').to(\"cuda\")\n",
    "m.eval()\n",
    "o = m.forward_features(torch.randn(2, 3, 384, 384).to(\"cuda\"))\n",
    "print(f'Unpooled shape: {o.shape}')\n",
    "# o = o.transpose(2,1)\n",
    "# print(f'Unpooled shape: {o.shape}')\n",
    "o = nn.AdaptiveAvgPool1d(1)(o.transpose(2,1))\n",
    "# o = o.mean(dim=[2, 3])\n",
    "print(f'Unpooled shape: {o.shape}')\n",
    "o = nn.Flatten(1)(o)\n",
    "print(f'Unpooled shape: {o.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b87172ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fd99e3e45c462d98ec538ab73bf9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273bf35366c2428ab6aa1d5ce6b487eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"klue/roberta-base\").to(\"cuda\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e55d5ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 5891, 2205, 5971, 1535, 2259, 6244, 2435, 2170, 4460, 2205, 2259,\n",
       "        4061, 2416, 2251, 3714,   18,    2,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "encoding = tokenizer.encode_plus(\n",
    "    \"안녕하세요 저는 데이콘에 참가하는 누구누구 입니다.\",\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    return_token_type_ids=False,\n",
    "    padding = 'max_length',\n",
    "    truncation = True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "display(encoding['input_ids'].flatten())\n",
    "display(encoding['attention_mask'].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "caed49cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d6a27d40ef4bc29b73272713fb02d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, ViTFeatureExtractor\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('./data/image/train/TRAIN_04463.jpg')\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-large-patch16-224')\n",
    "image_feature = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "image_feature['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a5566ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:798\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 798\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m    799\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "model(input_ids=torch.concat([encoding['input_ids'].flatten(), encoding['input_ids'].flatten()],dim=0), \n",
    "      attention_mask=torch.concat([encoding['attention_mask'].flatten(), encoding['attention_mask'].flatten()], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "175147e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA\n"
     ]
    }
   ],
   "source": [
    "a = False\n",
    "if not a :\n",
    "    print(\"AAA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83a596cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4, 76, 82])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1,2,3,4])\n",
    "b = torch.tensor([76,82])\n",
    "c = torch.tensor([-12])\n",
    "\n",
    "torch.cat([a,b], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "05c5cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1312, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.1312, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "a = torch.tensor([0.123], dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor([0], dtype=torch.float32)\n",
    "ce = nn.BCELoss()\n",
    "l = ce(a,b)\n",
    "print(l)\n",
    "l.backward()\n",
    "print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9cc83b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.745833333333332"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ 1.4938, 30.0000,  0.7437]) /3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cff20932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "a = [None, None, None]\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9fc6f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3, 14,  1, 12,  5, 11, 35])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor((1,2,3,4, 1,2,5,1,35))\n",
    "c = [3,7,5]\n",
    "a[c] += 10\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff44085c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.1320, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5715,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(16)\n",
    "# b = torch.ones(2)\n",
    "b = torch.randn(2)\n",
    "c = [1, 8]\n",
    "a[c] = b\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da7b2737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "a  = torch.tensor([]).to(\"cuda\")\n",
    "if not a.detach().cpu().numpy().tolist() :\n",
    "    print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d9b1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "if a is not None :\n",
    "    print(\"??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "966ec24f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m15\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "a = [False, 1, False, False, False, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "a.remove([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90449601",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if cat2_label14.shape[0] != 0 :\n",
    "        feature = feature[0:cat2_label14[0]]\n",
    "        feature = torch.cat([feature[0:cat2_label14[0]], feature[cat2_label14[0]+1 : ]])\n",
    "        for i in range(1, cat2_label14.shape[0]-1) :\n",
    "            feature = torch.cat([feature[cat2_label14[i] : cat2_label14[i+1]], feature[cat2_label14[i+1]+1 : ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b9e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fd6d22b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " ...]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca1_enc = {'자연': 0, '레포츠': 1, '음식': 2, '인문(문화/예술/역사)': 3, '숙박': 4, '쇼핑': 5}\n",
    "df = pd.concat([df['cat1'], df['cat2'], df['cat3']], axis=1)\n",
    "list(map(lambda x: ca1_enc[x], df['cat1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "68404449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cat1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [131]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcat1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcat2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcat1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m pd\u001b[38;5;241m.\u001b[39mmerge(df1, df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat3\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:107\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m--> 107\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:700\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cross \u001b[38;5;241m=\u001b[39m cross_col\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# note this function has side effects\u001b[39;00m\n\u001b[0;32m    696\u001b[0m (\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[1;32m--> 700\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_coerce_merge_keys()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1097\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_rkey(rk):\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1097\u001b[0m         right_keys\u001b[38;5;241m.\u001b[39mappend(\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label_or_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m         \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m         right_keys\u001b[38;5;241m.\u001b[39mappend(right\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\pandas\\core\\generic.py:1840\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1838\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mget_level_values(key)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cat1'"
     ]
    }
   ],
   "source": [
    "df1 = pd.merge(df['cat1'] , df['cat2'], on='cat1', how='left')\n",
    "pd.merge(df1, df['cat3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c4161b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.66666667, 6.66666667, 7.66666667, 8.66666667])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2,3,4], \n",
    "             [5,6,7,8,], \n",
    "             [11, 12, 13, 14]])\n",
    "np.mean(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4f057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "a = torch.randn(16,1024)\n",
    "for i in a :\n",
    "    o = nn.Linear(1024, 512)(i)\n",
    "    print(o.shape)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e8bb61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+04, 1.0000e+04, 2.1000e-01, 5.0000e-01, 1.0000e-01],\n",
       "        [1.0000e+04, 1.0000e+04, 2.1000e-01, 5.0000e-01, 1.0000e-01]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0.585, 0.662, 0.21, 0.5, 0.1], \n",
    "                 [0.585, 0.662, 0.21, 0.5, 0.1]])\n",
    "mask = torch.tensor([True, True, False, False, False])\n",
    "\n",
    "# a = torch.softmax(a, dim=0)\n",
    "a.masked_fill_(mask, 10000.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7799cffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "????\n"
     ]
    }
   ],
   "source": [
    "a = {1:0, 2:1}\n",
    "b = [v for v in a.values()]\n",
    "c = 2\n",
    "if c not in b :\n",
    "    print(\"????\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd1b3eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4235,  0.7278,  1.5660, -0.7307, -1.0090],\n",
       "        [ 1.8318, -0.3167,  1.8049,  0.2153, -0.1294]], requires_grad=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 5, requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80f6d6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.8500e-01,  6.6200e-01, -2.1000e-01,  5.0000e-01,  1.0000e-01],\n",
       "        [ 5.8500e-01,  2.1066e+02,  2.1000e-01, -5.0000e-01,  1.0000e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0.585, 0.662, -0.21, 0.5, 0.1], \n",
    "                 [0.585, 210.662, 0.21, -0.5, 0.1]], requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ec2412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module) :\n",
    "    def __init__(self, alpha=2, gamma=2, logits=False, reduction='none') :\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets) :\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction=self.reduction)(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction :\n",
    "            return torch.mean(F_loss)\n",
    "        else :\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7c951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1033,  0.2016,  0.7538, -0.2363,  0.4530],\n",
      "        [ 0.5181,  0.5696, -0.4533,  0.4932, -0.9827]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0000e+04,  2.0160e-01, -1.0000e+04, -2.3626e-01, -1.0000e+04],\n",
      "        [-1.0000e+04,  5.6958e-01, -1.0000e+04,  4.9323e-01, -1.0000e+04]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "model = nn.Linear(10, 5)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "model.train()\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "\n",
    "mask = torch.tensor([True, False, True, False, True], dtype=torch.bool)\n",
    "\n",
    "optim.zero_grad()\n",
    "data = torch.randn(2, 10)\n",
    "output = model(data)\n",
    "print(output)\n",
    "output.masked_fill_(mask, -10000)\n",
    "print(output)\n",
    "b = torch.tensor([1, 2])\n",
    "loss = FocalLoss()(output, b)\n",
    "\n",
    "loss.backward()\n",
    "optim.step()\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4cccc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2338, -0.2262,  0.1493,  0.0880, -0.2918,  0.1857,  0.2713,  0.2273,\n",
      "         -0.0548,  0.2618],\n",
      "        [ 0.0861, -0.2510, -0.0472, -0.0418, -0.1603,  0.2545,  0.0101, -0.2202,\n",
      "         -0.0835, -0.0616],\n",
      "        [-0.1318,  0.2218, -0.2585, -0.0276, -0.0871,  0.1444,  0.2138, -0.2815,\n",
      "          0.2514,  0.2036],\n",
      "        [-0.2373,  0.1889, -0.2306,  0.0869,  0.1074,  0.1208,  0.0529, -0.1172,\n",
      "         -0.0037,  0.0933],\n",
      "        [-0.0741, -0.0312,  0.1574, -0.1005, -0.0994,  0.3028,  0.2404, -0.2985,\n",
      "          0.1527,  0.1446]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3084, -0.0195,  0.2332,  0.2440,  0.1473], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2402, -0.1100,  0.1784,  0.1070,  0.1921,  0.0147, -0.2457, -0.1448,\n",
      "          0.3116, -0.0420],\n",
      "        [-0.1466, -0.0836,  0.2684,  0.0015, -0.2696, -0.0245, -0.1386, -0.1077,\n",
      "         -0.0422, -0.1907],\n",
      "        [ 0.0738, -0.1121,  0.1784,  0.0637, -0.3150,  0.0117,  0.1733,  0.0283,\n",
      "         -0.2719, -0.0309],\n",
      "        [-0.1781,  0.2134, -0.1927, -0.2858, -0.0561, -0.0142, -0.1028, -0.0222,\n",
      "          0.2568,  0.2319],\n",
      "        [-0.2552, -0.0859, -0.1171,  0.3029, -0.0125,  0.3086, -0.2073, -0.2552,\n",
      "         -0.3070, -0.1843]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0355, -0.1505,  0.1414,  0.2834, -0.2373], requires_grad=True)\n",
      "tensor([[ 2.1298, -0.4851,  0.3560, -0.4417,  0.6227],\n",
      "        [ 0.3980,  0.5394, -0.0298, -0.3731,  0.5695]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8800,  0.0566,  1.3688,  0.5083, -2.1980],\n",
      "        [ 0.6891,  0.1032,  0.2067,  0.3121, -0.6106]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "tensor([[-1.8741, -0.0274,  0.4873, -0.2245, -1.3687],\n",
      "        [ 0.2743,  0.0556, -0.0062, -0.1164, -0.3477]], grad_fn=<MulBackward0>)\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.2297, -0.1796,  0.1291,  0.0988, -0.2671,  0.1515,  0.2661,  0.2155,\n",
      "         -0.0639,  0.2429],\n",
      "        [ 0.0768, -0.2256, -0.0376, -0.0378, -0.1547,  0.2222,  0.0199, -0.1954,\n",
      "         -0.0777, -0.0465],\n",
      "        [-0.1116,  0.1834, -0.3202, -0.0131,  0.0483,  0.2543,  0.0546, -0.3215,\n",
      "          0.2417,  0.0501],\n",
      "        [-0.2227,  0.1811, -0.2313,  0.0910,  0.1240,  0.1273,  0.0280, -0.1162,\n",
      "         -0.0077,  0.0585],\n",
      "        [-0.0573, -0.0424,  0.1735, -0.1089, -0.1271,  0.2522,  0.2459, -0.2588,\n",
      "          0.1456,  0.1681]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2633, -0.0158,  0.1685,  0.2041,  0.1598], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2402, -0.1100,  0.1784,  0.1070,  0.1921,  0.0147, -0.2457, -0.1448,\n",
      "          0.3116, -0.0420],\n",
      "        [-0.1466, -0.0836,  0.2684,  0.0015, -0.2696, -0.0245, -0.1386, -0.1077,\n",
      "         -0.0422, -0.1907],\n",
      "        [ 0.0738, -0.1121,  0.1784,  0.0637, -0.3150,  0.0117,  0.1733,  0.0283,\n",
      "         -0.2719, -0.0309],\n",
      "        [-0.1781,  0.2134, -0.1927, -0.2858, -0.0561, -0.0142, -0.1028, -0.0222,\n",
      "          0.2568,  0.2319],\n",
      "        [-0.2552, -0.0859, -0.1171,  0.3029, -0.0125,  0.3086, -0.2073, -0.2552,\n",
      "         -0.3070, -0.1843]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0355, -0.1505,  0.1414,  0.2834, -0.2373], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "model = nn.Linear(10, 5)\n",
    "model2 = nn.Linear(10, 5)\n",
    "model2.eval()\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.9)\n",
    "model.train()\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "\n",
    "mask = torch.tensor([True, False, True, False, True], dtype=torch.bool)\n",
    "\n",
    "optim.zero_grad()\n",
    "data = torch.randn(2, 10)\n",
    "output = model(data)\n",
    "output2 = model2(data)\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "    \n",
    "for param in model2.parameters():\n",
    "    print(param)\n",
    "    \n",
    "    \n",
    "print(output)\n",
    "print(output2)\n",
    "print()\n",
    "output = output * output2\n",
    "print(output)\n",
    "print()\n",
    "b = torch.tensor([1, 2])\n",
    "loss = FocalLoss()(output, b)\n",
    "\n",
    "loss.backward()\n",
    "optim.step()\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "    \n",
    "for param in model2.parameters():\n",
    "    print(param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "89becf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1.585, 0.662, -0.21, 2.5, 0.1], \n",
    "                 [0.585, 2.662, 0.21, -0.5, 0.1]], requires_grad=True)\n",
    "a.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2465d816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(10, 3)\n",
    "a = torch.tensor([[True]* 3] * 10, dtype=torch.bool)\n",
    "t.masked_fill_(a, -1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e68b0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class LABEL_ENCODER() :\n",
    "    def __init__(self, path='./data/train.csv'):\n",
    "        self.base_encoder = self.base_label_encoder(path)\n",
    "\n",
    "    def base_label_encoder(self, path='./data/train.csv') :\n",
    "        df = pd.read_csv(path)\n",
    "        cat_encoder = {n_cat1: {} for n_cat1 in df['cat1'].unique()}\n",
    "        for n_cat1 in df['cat1'].unique():\n",
    "            cat2 = df[df['cat1'] == n_cat1]['cat2'].unique()\n",
    "\n",
    "            for n_cat2 in cat2:\n",
    "                cat3 = df[df['cat1'] == n_cat1][df['cat2'] == n_cat2]['cat3'].unique()\n",
    "                cat_encoder[n_cat1][n_cat2] = []\n",
    "\n",
    "                for n_cat3 in cat3:\n",
    "                    cat_encoder[n_cat1][n_cat2].append(n_cat3)\n",
    "        return cat_encoder\n",
    "\n",
    "    def cat1_label_index_encoder(self) :\n",
    "        cat1_encoder = {k: i for i, k in enumerate(self.base_encoder.keys())}\n",
    "        return cat1_encoder\n",
    "\n",
    "    def cat1_label_encoder(self):\n",
    "        return self.cat1_label_index_encoder()\n",
    "\n",
    "    def cat2_label_index_encoder(self) :\n",
    "        cat2_encoder = []\n",
    "        cnt = 0\n",
    "        for cat1_k in self.base_encoder.keys():\n",
    "            enc = {}\n",
    "            for cat2_k in self.base_encoder[cat1_k].keys() :\n",
    "                enc[cat2_k] = cnt\n",
    "                cnt += 1\n",
    "            cat2_encoder.append(enc)\n",
    "        return cat2_encoder\n",
    "\n",
    "    def cat2_label_encoder(self):\n",
    "        cat2_encoder = {}\n",
    "        cnt = 0\n",
    "        for cat1_k in self.base_encoder.keys():\n",
    "            for cat2_k in self.base_encoder[cat1_k].keys() :\n",
    "                cat2_encoder[cat2_k] = cnt\n",
    "                cnt += 1\n",
    "        return cat2_encoder\n",
    "\n",
    "    def cat3_label_index_encoder(self) :\n",
    "        cat3_encoder = []\n",
    "        cnt = 0\n",
    "        for cat1_k in self.base_encoder.keys():\n",
    "            for cat2_k in self.base_encoder[cat1_k].keys():\n",
    "                enc = {}\n",
    "                for cat3_k in self.base_encoder[cat1_k][cat2_k]:\n",
    "                    enc[cat3_k] = cnt\n",
    "                    cnt += 1\n",
    "                cat3_encoder.append(enc)\n",
    "        return cat3_encoder\n",
    "\n",
    "    def cat3_label_encoder(self) :\n",
    "        cat3_encoder = {}\n",
    "        cnt = 0\n",
    "        for cat1_k in self.base_encoder.keys():\n",
    "            for cat2_k in self.base_encoder[cat1_k].keys():\n",
    "                for cat3_k in self.base_encoder[cat1_k][cat2_k]:\n",
    "                    cat3_encoder[cat3_k] = cnt\n",
    "                    cnt += 1\n",
    "        return cat3_encoder\n",
    "\n",
    "class CATEGORY_MASKING() :\n",
    "    def __init__(self, path='./data/train.csv', device=\"cuda\"):\n",
    "        self.base_encoder = LABEL_ENCODER(path=path).base_encoder\n",
    "        self.cat1_encoder = self.cat1_cls_encoder()\n",
    "        self.cat2_encoder = self.cat2_cls_encoder()\n",
    "        self.cat3_encoder = self.cat3_cls_encoder()\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, pred, category):\n",
    "        # prev = tensor2list(prev) if prev is not None else None\n",
    "        batch_size = pred.shape[0]\n",
    "        pred = tensor2list(pred)\n",
    "\n",
    "        if category == 1:\n",
    "            mask = torch.tensor([[True] * 18] * batch_size, dtype=torch.bool)\n",
    "            for idx, c in enumerate(pred) :\n",
    "                mask[idx][list(self.cat2_encoder[c].values())] = False\n",
    "            return mask\n",
    "\n",
    "        elif category == 2:\n",
    "            mask = torch.tensor([[True] * 128] * batch_size, dtype=torch.bool)\n",
    "            for idx, c in enumerate(pred) :\n",
    "                mask[idx][list(self.cat3_encoder[c].values())] = False\n",
    "            return mask\n",
    "\n",
    "    def cat1_cls_encoder(self) :\n",
    "        # cat1_encoder = {0:0, 1:1, 2:5, 3:2, 4:14, 5:15}\n",
    "        cat1_encoder = [0, 1, 2, 3, 4, 5]\n",
    "        return cat1_encoder\n",
    "\n",
    "    def cat2_cls_encoder(self) :\n",
    "        cat2_encoder = []\n",
    "        cnt = 0\n",
    "        for cat1_k in self.base_encoder.keys():\n",
    "            cat2 = {}\n",
    "            for i, cat2_k in enumerate(self.base_encoder[cat1_k].keys()) :\n",
    "                cat2[i] = cnt\n",
    "                cnt += 1\n",
    "            cat2_encoder.append(cat2)\n",
    "        return cat2_encoder\n",
    "\n",
    "    def cat3_cls_encoder(self) :\n",
    "        cat3_encoder = []\n",
    "        cnt = 0\n",
    "        for cat1_k in self.base_encoder.keys():\n",
    "            for cat2_k in self.base_encoder[cat1_k].keys():\n",
    "                cat3 = {}\n",
    "                for i, cat3_k in enumerate(self.base_encoder[cat1_k][cat2_k]) :\n",
    "                    cat3[i] = cnt\n",
    "                    cnt+=1\n",
    "                cat3_encoder.append(cat3)\n",
    "        return cat3_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "93c97ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2list(x):\n",
    "    return x.detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7765aab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quhb2\\AppData\\Local\\Temp\\ipykernel_19768\\840864657.py:13: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  cat3 = df[df['cat1'] == n_cat1][df['cat2'] == n_cat2]['cat3'].unique()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 4, 2, 1, 4, 3, 4, 0, 2, 1, 3, 3, 5, 2, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True, False,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True, False,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True, False,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "         False, False, False, False, False, False,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True, False,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "         False, False, False, False, False, False,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "         False, False, False, False, False, False,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True, False]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-7.3429e-01, -4.4208e-01, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -8.1232e-01, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -2.0983e-01, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -5.2688e-01, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04,  2.8654e-02,  6.0129e-01,  4.6343e-01,\n",
       "          1.3301e+00,  1.9442e-01, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04,  1.3278e+00, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04,  1.1541e+00, -2.0831e+00,\n",
       "          6.1091e-01, -2.9701e-01,  1.5504e+00,  2.0893e+00, -3.8237e-01,\n",
       "          8.4938e-01, -1.0000e+04, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04,  1.1506e+00, -1.0000e+04],\n",
       "        [-1.7523e-01, -2.3510e-01, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -3.2809e-02, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.1350e+00,  1.1426e+00, -1.4336e+00,\n",
       "         -4.9507e-01, -7.5032e-01, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04,  9.6624e-01, -4.1935e-01,\n",
       "         -1.5237e+00,  8.0710e-01,  4.1653e-02,  8.6273e-01, -2.2824e+00,\n",
       "          6.5070e-01, -1.0000e+04, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04,  9.7016e-01,  1.0919e+00,\n",
       "          8.0422e-01,  2.5463e-01,  7.4092e-01, -7.1398e-01,  2.6782e-01,\n",
       "          8.3567e-01, -1.0000e+04, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -4.6598e-01],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -9.4591e-01, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04],\n",
       "        [-1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04, -1.0000e+04,\n",
       "         -1.0000e+04, -1.0000e+04, -9.0480e-01]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = CATEGORY_MASKING()\n",
    "a = torch.randn(16, 6, requires_grad=True)\n",
    "b = torch.randn(16, 18)\n",
    "display(a.argmax(1))\n",
    "mask = cm(a.argmax(1), 1)\n",
    "display(cm(a.argmax(1), 1))\n",
    "b = b.masked_fill_(mask, -10000)\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "797a5e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\cat1_label_weight.npy',\n",
       " 'data\\\\cat2_label_weight.npy',\n",
       " 'data\\\\cat3_label_weight.npy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "a = 'data/*_label_weight.npy'\n",
    "a = glob(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0b1cf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00697085, 0.00480769, 0.00223941, 0.01933216, 0.00700191,\n",
       "       0.00901639, 0.01237345, 0.00767085, 0.07913669, 0.0158046 ,\n",
       "       0.0136646 , 0.45833333, 0.03353659, 0.07801418, 0.03971119,\n",
       "       0.11827957, 1.        , 1.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.load(a[1], allow_pickle=True)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d009939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0103, 0.1295, 0.0536, 0.2900, 0.0771, 0.4395],\n",
       "        [0.4083, 0.1453, 0.0507, 0.0571, 0.1141, 0.2245],\n",
       "        [0.4935, 0.1777, 0.0973, 0.0533, 0.1043, 0.0739],\n",
       "        [0.1160, 0.3136, 0.0771, 0.0915, 0.2413, 0.1605],\n",
       "        [0.1678, 0.3958, 0.0852, 0.2940, 0.0178, 0.0394],\n",
       "        [0.2088, 0.1183, 0.1516, 0.3304, 0.0898, 0.1010],\n",
       "        [0.0867, 0.0290, 0.0642, 0.6013, 0.1058, 0.1129],\n",
       "        [0.2236, 0.1836, 0.0902, 0.0641, 0.3100, 0.1285],\n",
       "        [0.0748, 0.0234, 0.4564, 0.1702, 0.0294, 0.2458],\n",
       "        [0.1572, 0.1951, 0.0931, 0.3043, 0.0607, 0.1896],\n",
       "        [0.1410, 0.0250, 0.3279, 0.0114, 0.1019, 0.3928],\n",
       "        [0.0901, 0.0172, 0.0332, 0.1836, 0.0631, 0.6128],\n",
       "        [0.0620, 0.1831, 0.4555, 0.0504, 0.0474, 0.2015],\n",
       "        [0.1650, 0.0734, 0.0520, 0.2605, 0.0947, 0.3544],\n",
       "        [0.1656, 0.2115, 0.1013, 0.2099, 0.1236, 0.1882],\n",
       "        [0.1685, 0.0571, 0.0635, 0.1716, 0.2889, 0.2504]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "_a = torch.randn(16, 6)\n",
    "_asm = _a.softmax(dim=1)\n",
    "_asm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a825efa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2011, 0.0424, 0.0385, 0.0383, 0.0904],\n",
      "        [0.0151, 0.1258, 0.1234, 0.4237, 0.0948],\n",
      "        [0.2656, 0.0225, 0.3656, 0.0534, 0.0808],\n",
      "        [0.1220, 0.2838, 0.1233, 0.1792, 0.0301],\n",
      "        [0.2720, 0.2028, 0.0786, 0.1095, 0.0772],\n",
      "        [0.0210, 0.2051, 0.2919, 0.0113, 0.0289],\n",
      "        [0.1237, 0.0859, 0.2102, 0.1228, 0.2964],\n",
      "        [0.0738, 0.1188, 0.1227, 0.1822, 0.0442],\n",
      "        [0.0397, 0.0052, 0.1554, 0.4956, 0.0154],\n",
      "        [0.1483, 0.0456, 0.0163, 0.1873, 0.3249],\n",
      "        [0.1239, 0.0661, 0.0169, 0.4065, 0.1285],\n",
      "        [0.1752, 0.2091, 0.0799, 0.1502, 0.1000],\n",
      "        [0.0169, 0.0715, 0.0440, 0.2267, 0.0828],\n",
      "        [0.2872, 0.0547, 0.0238, 0.2746, 0.1116],\n",
      "        [0.0057, 0.0842, 0.0738, 0.5374, 0.2003],\n",
      "        [0.0047, 0.0561, 0.0589, 0.0060, 0.0529]])\n",
      "\n",
      "tensor([0.1442, 0.0716, 0.1575, 0.1276, 0.1245, 0.1193, 0.1074, 0.1009, 0.1631,\n",
      "        0.1180, 0.0396, 0.0910, 0.1509, 0.0684, 0.0949, 0.0718])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(16, 6)\n",
    "asm = a.softmax(dim=1)\n",
    "\n",
    "from_a_mask = torch.tensor([[True, True, False, False, True, False, True, True]]*16, dtype=torch.bool)\n",
    "b = torch.randn(16, 8)\n",
    "\n",
    "b = b.softmax(dim=1)\n",
    "# print(b)\n",
    "masked_b_t = b * from_a_mask\n",
    "nonz_b_t = torch.nonzero(masked_b_t, as_tuple=True)\n",
    "nonzt_b = b[nonz_b_t].reshape(b.shape[0], -1)\n",
    "print(nonzt_b)\n",
    "print()\n",
    "print(torch.mean(weight_b, dim=1))\n",
    "\n",
    "\n",
    "\n",
    "# c = a * b\n",
    "# print(c)\n",
    "# nonz_c = torch.nonzero(c)\n",
    "# print(c[nonz_c])\n",
    "# c = torch.sum(c[nonz_c[0]][nonz_c[1]], dim=1) / nonz_c.size(0)\n",
    "# print(c, nonz_c.size(0))\n",
    "# print()\n",
    "\n",
    "# d = a * torch.logical_not(b)\n",
    "# print(d)\n",
    "# nonz_d = torch.nonzero(d)\n",
    "# d = torch.sum(d[nonz_d]) / nonz_d.size(0)\n",
    "# print(d, nonz_d.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d8906af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4375, 0.0626, 0.1451, 0.2296, 0.0473, 0.0779])\n",
      "tensor([[0.4375],\n",
      "        [0.0626],\n",
      "        [0.0473],\n",
      "        [0.0779]])\n",
      "tensor([[0.1451],\n",
      "        [0.2296]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(6)\n",
    "b = torch.tensor([True, True, False, False, True, True], dtype=torch.bool)\n",
    "\n",
    "a = a.softmax(dim=0)\n",
    "print(a)\n",
    "c = a[torch.nonzero(a * b)]\n",
    "print(c)\n",
    "\n",
    "d = a[torch.nonzero(a * torch.logical_not(b))]\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fad5ce6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1732e+00, -1.2969e+00, -6.9498e-01,  ...,  5.4397e-01,\n",
       "           2.4270e-01,  9.6483e-01],\n",
       "         [-5.4548e-02,  5.2748e-01,  3.4845e-01,  ...,  1.1875e-01,\n",
       "           9.3034e-01,  8.0774e-01],\n",
       "         [-6.1922e-01,  1.1978e-01, -1.0172e+00,  ..., -8.0933e-02,\n",
       "          -1.9113e+00,  4.2812e-01],\n",
       "         [-4.1650e-01, -8.9483e-02, -6.7616e-01,  ...,  6.1290e-01,\n",
       "          -1.9259e+00,  2.3403e+00],\n",
       "         [ 8.7580e-01, -3.7656e-01, -2.9290e+00,  ...,  5.2533e-01,\n",
       "           9.7770e-01,  1.2055e+00],\n",
       "         [-6.6943e-01,  3.0507e-01, -1.6694e+00,  ..., -5.2178e-01,\n",
       "           1.2379e+00,  4.1653e-01]],\n",
       "\n",
       "        [[ 5.4728e-01, -7.0616e-01, -9.9050e-01,  ...,  2.5923e-01,\n",
       "           6.3574e-01,  1.5961e+00],\n",
       "         [-1.5749e+00, -7.2822e-01, -1.7259e+00,  ...,  5.9919e-01,\n",
       "          -4.5433e-01, -6.2902e-01],\n",
       "         [-3.4856e-01,  6.6874e-01,  4.9324e-01,  ..., -1.5901e-01,\n",
       "           1.0171e+00, -2.0087e-01],\n",
       "         [ 1.2062e+00, -9.6634e-01,  3.0644e-01,  ..., -2.2661e+00,\n",
       "          -4.0724e-01, -1.4156e+00],\n",
       "         [ 1.1540e+00,  8.9758e-01, -6.7594e-01,  ..., -1.3045e+00,\n",
       "          -8.2628e-02,  6.9783e-02],\n",
       "         [-1.4016e+00,  3.5217e-01,  4.2806e-01,  ...,  8.5482e-01,\n",
       "           7.7079e-01, -2.0274e-01]],\n",
       "\n",
       "        [[ 1.4131e+00, -3.1076e-01,  1.4552e+00,  ..., -6.9589e-02,\n",
       "          -1.0226e+00,  5.4950e-01],\n",
       "         [-1.3205e+00,  8.0643e-01,  4.0639e-01,  ..., -1.5496e+00,\n",
       "           1.1738e+00, -7.7027e-01],\n",
       "         [-6.3975e-01, -7.3326e-02, -6.4557e-01,  ...,  3.1485e-01,\n",
       "          -1.4671e+00,  5.8824e-01],\n",
       "         [ 2.7259e-01, -6.6425e-01,  3.6508e-01,  ...,  1.0679e+00,\n",
       "          -2.2203e+00, -6.0090e-02],\n",
       "         [ 1.1071e+00,  1.1163e+00,  4.2185e-01,  ...,  1.7238e+00,\n",
       "           4.0597e-01,  1.3933e+00],\n",
       "         [-9.2209e-01, -1.4257e-01, -3.3405e-02,  ...,  4.4139e-01,\n",
       "          -3.0308e+00, -1.6420e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.4328e-01,  1.0852e+00,  2.4380e-01,  ...,  8.5280e-01,\n",
       "          -1.4641e+00,  1.0913e+00],\n",
       "         [ 1.0062e-01, -5.6608e-01, -8.4518e-01,  ...,  5.3527e-01,\n",
       "          -1.2205e+00, -4.5278e-01],\n",
       "         [-1.0274e+00,  4.2747e-01, -1.2101e+00,  ..., -2.3275e-02,\n",
       "          -1.1994e+00, -2.6329e-02],\n",
       "         [-3.2550e-01,  2.1239e+00,  1.1569e+00,  ..., -1.9022e-01,\n",
       "           1.0606e+00, -1.0460e+00],\n",
       "         [ 9.9690e-01,  1.3553e+00, -2.0715e+00,  ..., -7.6525e-01,\n",
       "           3.7740e-01, -4.4337e-01],\n",
       "         [-4.5667e-03, -1.8126e+00,  5.4134e-01,  ..., -1.2819e-01,\n",
       "           1.0259e+00, -8.3145e-01]],\n",
       "\n",
       "        [[ 1.7586e+00,  3.8309e-01,  1.0844e+00,  ...,  8.6938e-01,\n",
       "          -8.4767e-01,  7.6977e-03],\n",
       "         [-9.3014e-01, -4.1360e-01,  4.6698e-01,  ..., -1.1759e+00,\n",
       "           1.0290e+00, -1.2499e-01],\n",
       "         [ 3.1197e-01, -1.2779e+00, -1.0401e-01,  ...,  3.5028e-01,\n",
       "           1.5722e-02,  4.4761e-01],\n",
       "         [ 8.9238e-01, -1.4534e-01, -1.0276e+00,  ...,  1.3904e+00,\n",
       "           1.3502e+00, -2.2288e-01],\n",
       "         [-3.5196e-01, -3.9325e-01,  1.2440e+00,  ..., -6.2957e-01,\n",
       "          -1.1533e+00,  8.3178e-01],\n",
       "         [-4.2749e-01, -1.0972e+00,  1.1319e+00,  ..., -7.8728e-01,\n",
       "          -8.8037e-01,  3.2095e-01]],\n",
       "\n",
       "        [[ 6.9050e-01, -5.5784e-01, -9.7481e-03,  ..., -1.0731e+00,\n",
       "          -5.0396e-01,  1.4022e-01],\n",
       "         [ 4.2377e-01, -1.4150e+00, -6.0708e-05,  ..., -1.7462e+00,\n",
       "           4.4890e-01, -3.8742e-02],\n",
       "         [-5.1429e-01, -1.3927e+00,  2.7994e-01,  ..., -9.5615e-01,\n",
       "           4.2239e-02, -6.5853e-01],\n",
       "         [ 1.0815e+00,  3.3013e-02, -6.4039e-01,  ..., -5.6414e-01,\n",
       "          -1.0979e+00,  1.1689e+00],\n",
       "         [-5.2976e-01, -8.1460e-01,  1.0150e+00,  ..., -1.3353e+00,\n",
       "           1.6781e+00,  5.9200e-02],\n",
       "         [ 1.7497e+00,  6.0475e-01,  6.2626e-01,  ..., -1.7857e-01,\n",
       "           1.3228e+00,  9.0720e-01]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(16, 6, 18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f45f6af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1886, -0.1756,  0.1828, -1.8382,  0.5449,  2.3116],\n",
       "        [-1.4165,  0.9522,  1.9263, -1.7340, -0.3287,  0.5425],\n",
       "        [ 0.6463,  1.2064, -0.7925,  1.3333,  0.6884,  0.9687],\n",
       "        [-1.2577,  0.0034,  1.1171,  1.4628,  0.7016,  0.8896],\n",
       "        [ 0.4332,  0.1903, -0.2327, -2.3196,  0.6093, -0.8570],\n",
       "        [-1.0063,  0.1774, -0.8022, -1.2216,  0.1261, -0.6508],\n",
       "        [-0.0535, -1.4382, -0.5517, -0.3010, -1.6803, -0.6080],\n",
       "        [-1.4398, -0.7219, -0.4901,  0.2340, -2.6927,  1.0121],\n",
       "        [ 0.3964,  0.2806,  0.2092, -0.2366,  0.0251,  0.4198],\n",
       "        [-0.8794,  1.3100,  0.3129,  0.1460, -0.1678,  0.0137],\n",
       "        [ 1.0535, -0.9076, -0.9517,  1.2907,  0.3593, -0.1760],\n",
       "        [-0.6529, -0.0996,  2.1730, -0.8755,  0.0835, -0.9575],\n",
       "        [-0.6150, -1.8078,  1.4008, -0.9616,  0.0797,  0.2226],\n",
       "        [ 0.5488,  0.4283,  0.2186, -0.3989, -1.2420,  1.4642],\n",
       "        [ 1.5740,  1.6295, -0.5486, -0.4446, -0.0559, -0.7566],\n",
       "        [-0.0510, -1.0136,  0.4527,  0.3649,  0.4070, -1.4993]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "526c810f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1886, -1.4165,  0.6463, -1.2577,  0.4332, -1.0063, -0.0535, -1.4398,\n",
       "         0.3964, -0.8794,  1.0535, -0.6529, -0.6150,  0.5488,  1.5740, -0.0510])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b236b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.7198)\n",
      "torch.Size([100])\n",
      "tensor([[ 90.5575,   5.0583,   6.4521,  27.4604,   2.6455,  -8.9076,  18.3370,\n",
      "           4.4131,   1.9847,   7.7713],\n",
      "        [  5.0583, 105.5826,  -1.7088,   1.4740,   8.7716,  25.4348,  -6.0144,\n",
      "          -6.3121,   4.1721,   9.0292],\n",
      "        [  6.4521,  -1.7088,  82.2148,  -7.2644,  17.9593,  -7.7805,  -8.5364,\n",
      "           3.2677, -14.8122,  -1.4943],\n",
      "        [ 27.4604,   1.4740,  -7.2644, 129.0732, -18.5602,   0.2304,   8.0666,\n",
      "         -20.6895,  -0.2049,   7.3049],\n",
      "        [  2.6455,   8.7716,  17.9593, -18.5602, 132.8260,   9.4371,   6.8716,\n",
      "           2.0847,  -1.2798, -13.5767],\n",
      "        [ -8.9076,  25.4348,  -7.7805,   0.2304,   9.4371, 111.2436, -12.9222,\n",
      "           1.8347, -11.6514,  12.0855],\n",
      "        [ 18.3370,  -6.0144,  -8.5364,   8.0666,   6.8716, -12.9222, 111.6609,\n",
      "           9.9300,  -1.1839,  15.7811],\n",
      "        [  4.4131,  -6.3121,   3.2677, -20.6895,   2.0847,   1.8347,   9.9300,\n",
      "          98.7845,  -3.8432,   7.3481],\n",
      "        [  1.9847,   4.1721, -14.8122,  -0.2049,  -1.2798, -11.6514,  -1.1839,\n",
      "          -3.8432,  83.0867, -18.6220],\n",
      "        [  7.7713,   9.0292,  -1.4943,   7.3049, -13.5767,  12.0855,  15.7811,\n",
      "           7.3481, -18.6220,  85.5736]])\n"
     ]
    }
   ],
   "source": [
    "m = torch.randn(100,10)\n",
    "v = torch.randn(10)\n",
    "d = torch.matmul(v,v) # = torch.dot, 벡터의 내적\n",
    "print(d)\n",
    "v2 = torch.matmul(m,v) # = torch.mv, 행렬과 벡터의 곱\n",
    "print(v2.shape)\n",
    "m2 = torch.matmul(m.t(), m) # = torch.mm, 행렬 곱\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9d434e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3476,  1.4442,  0.6515,  0.3200,  1.5556, -0.7941,  0.1368, -0.1559,\n",
       "         0.2006,  1.4702,  1.0685,  0.7629,  0.8333, -0.7893, -0.0929, -1.6456])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(16)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c2851779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 16])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([list(a.numpy())]*18).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f9c23187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 10, 18])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1a1f5c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(list(np.load('./data/cat3_label_weight.npy', allow_pickle=True)))\n",
    "cel = nn.CrossEntropyLoss(weight=a)\n",
    "cel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3aa9ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2tensor(x, dtype=torch.float, device=\"cuda\"):\n",
    "    return torch.tensor(x, dtype=dtype).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83b80e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "a = [1,2,3,4,5,6]\n",
    "c = [random.choice(a) for _ in range(len(a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b90780d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>한식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>한식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>한식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>수련시설</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>산</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7275</th>\n",
       "      <td>TEST_07275</td>\n",
       "      <td>한식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7276</th>\n",
       "      <td>TEST_07276</td>\n",
       "      <td>상설시장</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7277</th>\n",
       "      <td>TEST_07277</td>\n",
       "      <td>야영장,오토캠핑장</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7278</th>\n",
       "      <td>TEST_07278</td>\n",
       "      <td>모텔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7279</th>\n",
       "      <td>TEST_07279</td>\n",
       "      <td>자연휴양림</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7280 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id       cat3\n",
       "0     TEST_00000         한식\n",
       "1     TEST_00001         한식\n",
       "2     TEST_00002         한식\n",
       "3     TEST_00003       수련시설\n",
       "4     TEST_00004          산\n",
       "...          ...        ...\n",
       "7275  TEST_07275         한식\n",
       "7276  TEST_07276       상설시장\n",
       "7277  TEST_07277  야영장,오토캠핑장\n",
       "7278  TEST_07278         모텔\n",
       "7279  TEST_07279      자연휴양림\n",
       "\n",
       "[7280 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = pd.read_csv('./submission/29E-val0.8145-head1-2mask-lossWeight-auged-effiv2s.csv')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b51e6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1232'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(round(0.1232345, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adf61691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/image/test\\\\TEST_00000.jpg',\n",
       " './data/image/test\\\\TEST_00001.jpg',\n",
       " './data/image/test\\\\TEST_00002.jpg',\n",
       " './data/image/test\\\\TEST_00003.jpg',\n",
       " './data/image/test\\\\TEST_00004.jpg',\n",
       " './data/image/test\\\\TEST_00005.jpg',\n",
       " './data/image/test\\\\TEST_00006.jpg',\n",
       " './data/image/test\\\\TEST_00007.jpg',\n",
       " './data/image/test\\\\TEST_00008.jpg',\n",
       " './data/image/test\\\\TEST_00009.jpg',\n",
       " './data/image/test\\\\TEST_00010.jpg',\n",
       " './data/image/test\\\\TEST_00011.jpg',\n",
       " './data/image/test\\\\TEST_00012.jpg',\n",
       " './data/image/test\\\\TEST_00013.jpg',\n",
       " './data/image/test\\\\TEST_00014.jpg',\n",
       " './data/image/test\\\\TEST_00015.jpg',\n",
       " './data/image/test\\\\TEST_00016.jpg',\n",
       " './data/image/test\\\\TEST_00017.jpg',\n",
       " './data/image/test\\\\TEST_00018.jpg',\n",
       " './data/image/test\\\\TEST_00019.jpg',\n",
       " './data/image/test\\\\TEST_00020.jpg',\n",
       " './data/image/test\\\\TEST_00021.jpg',\n",
       " './data/image/test\\\\TEST_00022.jpg',\n",
       " './data/image/test\\\\TEST_00023.jpg',\n",
       " './data/image/test\\\\TEST_00024.jpg',\n",
       " './data/image/test\\\\TEST_00025.jpg',\n",
       " './data/image/test\\\\TEST_00026.jpg',\n",
       " './data/image/test\\\\TEST_00027.jpg',\n",
       " './data/image/test\\\\TEST_00028.jpg',\n",
       " './data/image/test\\\\TEST_00029.jpg',\n",
       " './data/image/test\\\\TEST_00030.jpg',\n",
       " './data/image/test\\\\TEST_00031.jpg',\n",
       " './data/image/test\\\\TEST_00032.jpg',\n",
       " './data/image/test\\\\TEST_00033.jpg',\n",
       " './data/image/test\\\\TEST_00034.jpg',\n",
       " './data/image/test\\\\TEST_00035.jpg',\n",
       " './data/image/test\\\\TEST_00036.jpg',\n",
       " './data/image/test\\\\TEST_00037.jpg',\n",
       " './data/image/test\\\\TEST_00038.jpg',\n",
       " './data/image/test\\\\TEST_00039.jpg',\n",
       " './data/image/test\\\\TEST_00040.jpg',\n",
       " './data/image/test\\\\TEST_00041.jpg',\n",
       " './data/image/test\\\\TEST_00042.jpg',\n",
       " './data/image/test\\\\TEST_00043.jpg',\n",
       " './data/image/test\\\\TEST_00044.jpg',\n",
       " './data/image/test\\\\TEST_00045.jpg',\n",
       " './data/image/test\\\\TEST_00046.jpg',\n",
       " './data/image/test\\\\TEST_00047.jpg',\n",
       " './data/image/test\\\\TEST_00048.jpg',\n",
       " './data/image/test\\\\TEST_00049.jpg',\n",
       " './data/image/test\\\\TEST_00050.jpg',\n",
       " './data/image/test\\\\TEST_00051.jpg',\n",
       " './data/image/test\\\\TEST_00052.jpg',\n",
       " './data/image/test\\\\TEST_00053.jpg',\n",
       " './data/image/test\\\\TEST_00054.jpg',\n",
       " './data/image/test\\\\TEST_00055.jpg',\n",
       " './data/image/test\\\\TEST_00056.jpg',\n",
       " './data/image/test\\\\TEST_00057.jpg',\n",
       " './data/image/test\\\\TEST_00058.jpg',\n",
       " './data/image/test\\\\TEST_00059.jpg',\n",
       " './data/image/test\\\\TEST_00060.jpg',\n",
       " './data/image/test\\\\TEST_00061.jpg',\n",
       " './data/image/test\\\\TEST_00062.jpg',\n",
       " './data/image/test\\\\TEST_00063.jpg',\n",
       " './data/image/test\\\\TEST_00064.jpg',\n",
       " './data/image/test\\\\TEST_00065.jpg',\n",
       " './data/image/test\\\\TEST_00066.jpg',\n",
       " './data/image/test\\\\TEST_00067.jpg',\n",
       " './data/image/test\\\\TEST_00068.jpg',\n",
       " './data/image/test\\\\TEST_00069.jpg',\n",
       " './data/image/test\\\\TEST_00070.jpg',\n",
       " './data/image/test\\\\TEST_00071.jpg',\n",
       " './data/image/test\\\\TEST_00072.jpg',\n",
       " './data/image/test\\\\TEST_00073.jpg',\n",
       " './data/image/test\\\\TEST_00074.jpg',\n",
       " './data/image/test\\\\TEST_00075.jpg',\n",
       " './data/image/test\\\\TEST_00076.jpg',\n",
       " './data/image/test\\\\TEST_00077.jpg',\n",
       " './data/image/test\\\\TEST_00078.jpg',\n",
       " './data/image/test\\\\TEST_00079.jpg',\n",
       " './data/image/test\\\\TEST_00080.jpg',\n",
       " './data/image/test\\\\TEST_00081.jpg',\n",
       " './data/image/test\\\\TEST_00082.jpg',\n",
       " './data/image/test\\\\TEST_00083.jpg',\n",
       " './data/image/test\\\\TEST_00084.jpg',\n",
       " './data/image/test\\\\TEST_00085.jpg',\n",
       " './data/image/test\\\\TEST_00086.jpg',\n",
       " './data/image/test\\\\TEST_00087.jpg',\n",
       " './data/image/test\\\\TEST_00088.jpg',\n",
       " './data/image/test\\\\TEST_00089.jpg',\n",
       " './data/image/test\\\\TEST_00090.jpg',\n",
       " './data/image/test\\\\TEST_00091.jpg',\n",
       " './data/image/test\\\\TEST_00092.jpg',\n",
       " './data/image/test\\\\TEST_00093.jpg',\n",
       " './data/image/test\\\\TEST_00094.jpg',\n",
       " './data/image/test\\\\TEST_00095.jpg',\n",
       " './data/image/test\\\\TEST_00096.jpg',\n",
       " './data/image/test\\\\TEST_00097.jpg',\n",
       " './data/image/test\\\\TEST_00098.jpg',\n",
       " './data/image/test\\\\TEST_00099.jpg',\n",
       " './data/image/test\\\\TEST_00100.jpg',\n",
       " './data/image/test\\\\TEST_00101.jpg',\n",
       " './data/image/test\\\\TEST_00102.jpg',\n",
       " './data/image/test\\\\TEST_00103.jpg',\n",
       " './data/image/test\\\\TEST_00104.jpg',\n",
       " './data/image/test\\\\TEST_00105.jpg',\n",
       " './data/image/test\\\\TEST_00106.jpg',\n",
       " './data/image/test\\\\TEST_00107.jpg',\n",
       " './data/image/test\\\\TEST_00108.jpg',\n",
       " './data/image/test\\\\TEST_00109.jpg',\n",
       " './data/image/test\\\\TEST_00110.jpg',\n",
       " './data/image/test\\\\TEST_00111.jpg',\n",
       " './data/image/test\\\\TEST_00112.jpg',\n",
       " './data/image/test\\\\TEST_00113.jpg',\n",
       " './data/image/test\\\\TEST_00114.jpg',\n",
       " './data/image/test\\\\TEST_00115.jpg',\n",
       " './data/image/test\\\\TEST_00116.jpg',\n",
       " './data/image/test\\\\TEST_00117.jpg',\n",
       " './data/image/test\\\\TEST_00118.jpg',\n",
       " './data/image/test\\\\TEST_00119.jpg',\n",
       " './data/image/test\\\\TEST_00120.jpg',\n",
       " './data/image/test\\\\TEST_00121.jpg',\n",
       " './data/image/test\\\\TEST_00122.jpg',\n",
       " './data/image/test\\\\TEST_00123.jpg',\n",
       " './data/image/test\\\\TEST_00124.jpg',\n",
       " './data/image/test\\\\TEST_00125.jpg',\n",
       " './data/image/test\\\\TEST_00126.jpg',\n",
       " './data/image/test\\\\TEST_00127.jpg',\n",
       " './data/image/test\\\\TEST_00128.jpg',\n",
       " './data/image/test\\\\TEST_00129.jpg',\n",
       " './data/image/test\\\\TEST_00130.jpg',\n",
       " './data/image/test\\\\TEST_00131.jpg',\n",
       " './data/image/test\\\\TEST_00132.jpg',\n",
       " './data/image/test\\\\TEST_00133.jpg',\n",
       " './data/image/test\\\\TEST_00134.jpg',\n",
       " './data/image/test\\\\TEST_00135.jpg',\n",
       " './data/image/test\\\\TEST_00136.jpg',\n",
       " './data/image/test\\\\TEST_00137.jpg',\n",
       " './data/image/test\\\\TEST_00138.jpg',\n",
       " './data/image/test\\\\TEST_00139.jpg',\n",
       " './data/image/test\\\\TEST_00140.jpg',\n",
       " './data/image/test\\\\TEST_00141.jpg',\n",
       " './data/image/test\\\\TEST_00142.jpg',\n",
       " './data/image/test\\\\TEST_00143.jpg',\n",
       " './data/image/test\\\\TEST_00144.jpg',\n",
       " './data/image/test\\\\TEST_00145.jpg',\n",
       " './data/image/test\\\\TEST_00146.jpg',\n",
       " './data/image/test\\\\TEST_00147.jpg',\n",
       " './data/image/test\\\\TEST_00148.jpg',\n",
       " './data/image/test\\\\TEST_00149.jpg',\n",
       " './data/image/test\\\\TEST_00150.jpg',\n",
       " './data/image/test\\\\TEST_00151.jpg',\n",
       " './data/image/test\\\\TEST_00152.jpg',\n",
       " './data/image/test\\\\TEST_00153.jpg',\n",
       " './data/image/test\\\\TEST_00154.jpg',\n",
       " './data/image/test\\\\TEST_00155.jpg',\n",
       " './data/image/test\\\\TEST_00156.jpg',\n",
       " './data/image/test\\\\TEST_00157.jpg',\n",
       " './data/image/test\\\\TEST_00158.jpg',\n",
       " './data/image/test\\\\TEST_00159.jpg',\n",
       " './data/image/test\\\\TEST_00160.jpg',\n",
       " './data/image/test\\\\TEST_00161.jpg',\n",
       " './data/image/test\\\\TEST_00162.jpg',\n",
       " './data/image/test\\\\TEST_00163.jpg',\n",
       " './data/image/test\\\\TEST_00164.jpg',\n",
       " './data/image/test\\\\TEST_00165.jpg',\n",
       " './data/image/test\\\\TEST_00166.jpg',\n",
       " './data/image/test\\\\TEST_00167.jpg',\n",
       " './data/image/test\\\\TEST_00168.jpg',\n",
       " './data/image/test\\\\TEST_00169.jpg',\n",
       " './data/image/test\\\\TEST_00170.jpg',\n",
       " './data/image/test\\\\TEST_00171.jpg',\n",
       " './data/image/test\\\\TEST_00172.jpg',\n",
       " './data/image/test\\\\TEST_00173.jpg',\n",
       " './data/image/test\\\\TEST_00174.jpg',\n",
       " './data/image/test\\\\TEST_00175.jpg',\n",
       " './data/image/test\\\\TEST_00176.jpg',\n",
       " './data/image/test\\\\TEST_00177.jpg',\n",
       " './data/image/test\\\\TEST_00178.jpg',\n",
       " './data/image/test\\\\TEST_00179.jpg',\n",
       " './data/image/test\\\\TEST_00180.jpg',\n",
       " './data/image/test\\\\TEST_00181.jpg',\n",
       " './data/image/test\\\\TEST_00182.jpg',\n",
       " './data/image/test\\\\TEST_00183.jpg',\n",
       " './data/image/test\\\\TEST_00184.jpg',\n",
       " './data/image/test\\\\TEST_00185.jpg',\n",
       " './data/image/test\\\\TEST_00186.jpg',\n",
       " './data/image/test\\\\TEST_00187.jpg',\n",
       " './data/image/test\\\\TEST_00188.jpg',\n",
       " './data/image/test\\\\TEST_00189.jpg',\n",
       " './data/image/test\\\\TEST_00190.jpg',\n",
       " './data/image/test\\\\TEST_00191.jpg',\n",
       " './data/image/test\\\\TEST_00192.jpg',\n",
       " './data/image/test\\\\TEST_00193.jpg',\n",
       " './data/image/test\\\\TEST_00194.jpg',\n",
       " './data/image/test\\\\TEST_00195.jpg',\n",
       " './data/image/test\\\\TEST_00196.jpg',\n",
       " './data/image/test\\\\TEST_00197.jpg',\n",
       " './data/image/test\\\\TEST_00198.jpg',\n",
       " './data/image/test\\\\TEST_00199.jpg',\n",
       " './data/image/test\\\\TEST_00200.jpg',\n",
       " './data/image/test\\\\TEST_00201.jpg',\n",
       " './data/image/test\\\\TEST_00202.jpg',\n",
       " './data/image/test\\\\TEST_00203.jpg',\n",
       " './data/image/test\\\\TEST_00204.jpg',\n",
       " './data/image/test\\\\TEST_00205.jpg',\n",
       " './data/image/test\\\\TEST_00206.jpg',\n",
       " './data/image/test\\\\TEST_00207.jpg',\n",
       " './data/image/test\\\\TEST_00208.jpg',\n",
       " './data/image/test\\\\TEST_00209.jpg',\n",
       " './data/image/test\\\\TEST_00210.jpg',\n",
       " './data/image/test\\\\TEST_00211.jpg',\n",
       " './data/image/test\\\\TEST_00212.jpg',\n",
       " './data/image/test\\\\TEST_00213.jpg',\n",
       " './data/image/test\\\\TEST_00214.jpg',\n",
       " './data/image/test\\\\TEST_00215.jpg',\n",
       " './data/image/test\\\\TEST_00216.jpg',\n",
       " './data/image/test\\\\TEST_00217.jpg',\n",
       " './data/image/test\\\\TEST_00218.jpg',\n",
       " './data/image/test\\\\TEST_00219.jpg',\n",
       " './data/image/test\\\\TEST_00220.jpg',\n",
       " './data/image/test\\\\TEST_00221.jpg',\n",
       " './data/image/test\\\\TEST_00222.jpg',\n",
       " './data/image/test\\\\TEST_00223.jpg',\n",
       " './data/image/test\\\\TEST_00224.jpg',\n",
       " './data/image/test\\\\TEST_00225.jpg',\n",
       " './data/image/test\\\\TEST_00226.jpg',\n",
       " './data/image/test\\\\TEST_00227.jpg',\n",
       " './data/image/test\\\\TEST_00228.jpg',\n",
       " './data/image/test\\\\TEST_00229.jpg',\n",
       " './data/image/test\\\\TEST_00230.jpg',\n",
       " './data/image/test\\\\TEST_00231.jpg',\n",
       " './data/image/test\\\\TEST_00232.jpg',\n",
       " './data/image/test\\\\TEST_00233.jpg',\n",
       " './data/image/test\\\\TEST_00234.jpg',\n",
       " './data/image/test\\\\TEST_00235.jpg',\n",
       " './data/image/test\\\\TEST_00236.jpg',\n",
       " './data/image/test\\\\TEST_00237.jpg',\n",
       " './data/image/test\\\\TEST_00238.jpg',\n",
       " './data/image/test\\\\TEST_00239.jpg',\n",
       " './data/image/test\\\\TEST_00240.jpg',\n",
       " './data/image/test\\\\TEST_00241.jpg',\n",
       " './data/image/test\\\\TEST_00242.jpg',\n",
       " './data/image/test\\\\TEST_00243.jpg',\n",
       " './data/image/test\\\\TEST_00244.jpg',\n",
       " './data/image/test\\\\TEST_00245.jpg',\n",
       " './data/image/test\\\\TEST_00246.jpg',\n",
       " './data/image/test\\\\TEST_00247.jpg',\n",
       " './data/image/test\\\\TEST_00248.jpg',\n",
       " './data/image/test\\\\TEST_00249.jpg',\n",
       " './data/image/test\\\\TEST_00250.jpg',\n",
       " './data/image/test\\\\TEST_00251.jpg',\n",
       " './data/image/test\\\\TEST_00252.jpg',\n",
       " './data/image/test\\\\TEST_00253.jpg',\n",
       " './data/image/test\\\\TEST_00254.jpg',\n",
       " './data/image/test\\\\TEST_00255.jpg',\n",
       " './data/image/test\\\\TEST_00256.jpg',\n",
       " './data/image/test\\\\TEST_00257.jpg',\n",
       " './data/image/test\\\\TEST_00258.jpg',\n",
       " './data/image/test\\\\TEST_00259.jpg',\n",
       " './data/image/test\\\\TEST_00260.jpg',\n",
       " './data/image/test\\\\TEST_00261.jpg',\n",
       " './data/image/test\\\\TEST_00262.jpg',\n",
       " './data/image/test\\\\TEST_00263.jpg',\n",
       " './data/image/test\\\\TEST_00264.jpg',\n",
       " './data/image/test\\\\TEST_00265.jpg',\n",
       " './data/image/test\\\\TEST_00266.jpg',\n",
       " './data/image/test\\\\TEST_00267.jpg',\n",
       " './data/image/test\\\\TEST_00268.jpg',\n",
       " './data/image/test\\\\TEST_00269.jpg',\n",
       " './data/image/test\\\\TEST_00270.jpg',\n",
       " './data/image/test\\\\TEST_00271.jpg',\n",
       " './data/image/test\\\\TEST_00272.jpg',\n",
       " './data/image/test\\\\TEST_00273.jpg',\n",
       " './data/image/test\\\\TEST_00274.jpg',\n",
       " './data/image/test\\\\TEST_00275.jpg',\n",
       " './data/image/test\\\\TEST_00276.jpg',\n",
       " './data/image/test\\\\TEST_00277.jpg',\n",
       " './data/image/test\\\\TEST_00278.jpg',\n",
       " './data/image/test\\\\TEST_00279.jpg',\n",
       " './data/image/test\\\\TEST_00280.jpg',\n",
       " './data/image/test\\\\TEST_00281.jpg',\n",
       " './data/image/test\\\\TEST_00282.jpg',\n",
       " './data/image/test\\\\TEST_00283.jpg',\n",
       " './data/image/test\\\\TEST_00284.jpg',\n",
       " './data/image/test\\\\TEST_00285.jpg',\n",
       " './data/image/test\\\\TEST_00286.jpg',\n",
       " './data/image/test\\\\TEST_00287.jpg',\n",
       " './data/image/test\\\\TEST_00288.jpg',\n",
       " './data/image/test\\\\TEST_00289.jpg',\n",
       " './data/image/test\\\\TEST_00290.jpg',\n",
       " './data/image/test\\\\TEST_00291.jpg',\n",
       " './data/image/test\\\\TEST_00292.jpg',\n",
       " './data/image/test\\\\TEST_00293.jpg',\n",
       " './data/image/test\\\\TEST_00294.jpg',\n",
       " './data/image/test\\\\TEST_00295.jpg',\n",
       " './data/image/test\\\\TEST_00296.jpg',\n",
       " './data/image/test\\\\TEST_00297.jpg',\n",
       " './data/image/test\\\\TEST_00298.jpg',\n",
       " './data/image/test\\\\TEST_00299.jpg',\n",
       " './data/image/test\\\\TEST_00300.jpg',\n",
       " './data/image/test\\\\TEST_00301.jpg',\n",
       " './data/image/test\\\\TEST_00302.jpg',\n",
       " './data/image/test\\\\TEST_00303.jpg',\n",
       " './data/image/test\\\\TEST_00304.jpg',\n",
       " './data/image/test\\\\TEST_00305.jpg',\n",
       " './data/image/test\\\\TEST_00306.jpg',\n",
       " './data/image/test\\\\TEST_00307.jpg',\n",
       " './data/image/test\\\\TEST_00308.jpg',\n",
       " './data/image/test\\\\TEST_00309.jpg',\n",
       " './data/image/test\\\\TEST_00310.jpg',\n",
       " './data/image/test\\\\TEST_00311.jpg',\n",
       " './data/image/test\\\\TEST_00312.jpg',\n",
       " './data/image/test\\\\TEST_00313.jpg',\n",
       " './data/image/test\\\\TEST_00314.jpg',\n",
       " './data/image/test\\\\TEST_00315.jpg',\n",
       " './data/image/test\\\\TEST_00316.jpg',\n",
       " './data/image/test\\\\TEST_00317.jpg',\n",
       " './data/image/test\\\\TEST_00318.jpg',\n",
       " './data/image/test\\\\TEST_00319.jpg',\n",
       " './data/image/test\\\\TEST_00320.jpg',\n",
       " './data/image/test\\\\TEST_00321.jpg',\n",
       " './data/image/test\\\\TEST_00322.jpg',\n",
       " './data/image/test\\\\TEST_00323.jpg',\n",
       " './data/image/test\\\\TEST_00324.jpg',\n",
       " './data/image/test\\\\TEST_00325.jpg',\n",
       " './data/image/test\\\\TEST_00326.jpg',\n",
       " './data/image/test\\\\TEST_00327.jpg',\n",
       " './data/image/test\\\\TEST_00328.jpg',\n",
       " './data/image/test\\\\TEST_00329.jpg',\n",
       " './data/image/test\\\\TEST_00330.jpg',\n",
       " './data/image/test\\\\TEST_00331.jpg',\n",
       " './data/image/test\\\\TEST_00332.jpg',\n",
       " './data/image/test\\\\TEST_00333.jpg',\n",
       " './data/image/test\\\\TEST_00334.jpg',\n",
       " './data/image/test\\\\TEST_00335.jpg',\n",
       " './data/image/test\\\\TEST_00336.jpg',\n",
       " './data/image/test\\\\TEST_00337.jpg',\n",
       " './data/image/test\\\\TEST_00338.jpg',\n",
       " './data/image/test\\\\TEST_00339.jpg',\n",
       " './data/image/test\\\\TEST_00340.jpg',\n",
       " './data/image/test\\\\TEST_00341.jpg',\n",
       " './data/image/test\\\\TEST_00342.jpg',\n",
       " './data/image/test\\\\TEST_00343.jpg',\n",
       " './data/image/test\\\\TEST_00344.jpg',\n",
       " './data/image/test\\\\TEST_00345.jpg',\n",
       " './data/image/test\\\\TEST_00346.jpg',\n",
       " './data/image/test\\\\TEST_00347.jpg',\n",
       " './data/image/test\\\\TEST_00348.jpg',\n",
       " './data/image/test\\\\TEST_00349.jpg',\n",
       " './data/image/test\\\\TEST_00350.jpg',\n",
       " './data/image/test\\\\TEST_00351.jpg',\n",
       " './data/image/test\\\\TEST_00352.jpg',\n",
       " './data/image/test\\\\TEST_00353.jpg',\n",
       " './data/image/test\\\\TEST_00354.jpg',\n",
       " './data/image/test\\\\TEST_00355.jpg',\n",
       " './data/image/test\\\\TEST_00356.jpg',\n",
       " './data/image/test\\\\TEST_00357.jpg',\n",
       " './data/image/test\\\\TEST_00358.jpg',\n",
       " './data/image/test\\\\TEST_00359.jpg',\n",
       " './data/image/test\\\\TEST_00360.jpg',\n",
       " './data/image/test\\\\TEST_00361.jpg',\n",
       " './data/image/test\\\\TEST_00362.jpg',\n",
       " './data/image/test\\\\TEST_00363.jpg',\n",
       " './data/image/test\\\\TEST_00364.jpg',\n",
       " './data/image/test\\\\TEST_00365.jpg',\n",
       " './data/image/test\\\\TEST_00366.jpg',\n",
       " './data/image/test\\\\TEST_00367.jpg',\n",
       " './data/image/test\\\\TEST_00368.jpg',\n",
       " './data/image/test\\\\TEST_00369.jpg',\n",
       " './data/image/test\\\\TEST_00370.jpg',\n",
       " './data/image/test\\\\TEST_00371.jpg',\n",
       " './data/image/test\\\\TEST_00372.jpg',\n",
       " './data/image/test\\\\TEST_00373.jpg',\n",
       " './data/image/test\\\\TEST_00374.jpg',\n",
       " './data/image/test\\\\TEST_00375.jpg',\n",
       " './data/image/test\\\\TEST_00376.jpg',\n",
       " './data/image/test\\\\TEST_00377.jpg',\n",
       " './data/image/test\\\\TEST_00378.jpg',\n",
       " './data/image/test\\\\TEST_00379.jpg',\n",
       " './data/image/test\\\\TEST_00380.jpg',\n",
       " './data/image/test\\\\TEST_00381.jpg',\n",
       " './data/image/test\\\\TEST_00382.jpg',\n",
       " './data/image/test\\\\TEST_00383.jpg',\n",
       " './data/image/test\\\\TEST_00384.jpg',\n",
       " './data/image/test\\\\TEST_00385.jpg',\n",
       " './data/image/test\\\\TEST_00386.jpg',\n",
       " './data/image/test\\\\TEST_00387.jpg',\n",
       " './data/image/test\\\\TEST_00388.jpg',\n",
       " './data/image/test\\\\TEST_00389.jpg',\n",
       " './data/image/test\\\\TEST_00390.jpg',\n",
       " './data/image/test\\\\TEST_00391.jpg',\n",
       " './data/image/test\\\\TEST_00392.jpg',\n",
       " './data/image/test\\\\TEST_00393.jpg',\n",
       " './data/image/test\\\\TEST_00394.jpg',\n",
       " './data/image/test\\\\TEST_00395.jpg',\n",
       " './data/image/test\\\\TEST_00396.jpg',\n",
       " './data/image/test\\\\TEST_00397.jpg',\n",
       " './data/image/test\\\\TEST_00398.jpg',\n",
       " './data/image/test\\\\TEST_00399.jpg',\n",
       " './data/image/test\\\\TEST_00400.jpg',\n",
       " './data/image/test\\\\TEST_00401.jpg',\n",
       " './data/image/test\\\\TEST_00402.jpg',\n",
       " './data/image/test\\\\TEST_00403.jpg',\n",
       " './data/image/test\\\\TEST_00404.jpg',\n",
       " './data/image/test\\\\TEST_00405.jpg',\n",
       " './data/image/test\\\\TEST_00406.jpg',\n",
       " './data/image/test\\\\TEST_00407.jpg',\n",
       " './data/image/test\\\\TEST_00408.jpg',\n",
       " './data/image/test\\\\TEST_00409.jpg',\n",
       " './data/image/test\\\\TEST_00410.jpg',\n",
       " './data/image/test\\\\TEST_00411.jpg',\n",
       " './data/image/test\\\\TEST_00412.jpg',\n",
       " './data/image/test\\\\TEST_00413.jpg',\n",
       " './data/image/test\\\\TEST_00414.jpg',\n",
       " './data/image/test\\\\TEST_00415.jpg',\n",
       " './data/image/test\\\\TEST_00416.jpg',\n",
       " './data/image/test\\\\TEST_00417.jpg',\n",
       " './data/image/test\\\\TEST_00418.jpg',\n",
       " './data/image/test\\\\TEST_00419.jpg',\n",
       " './data/image/test\\\\TEST_00420.jpg',\n",
       " './data/image/test\\\\TEST_00421.jpg',\n",
       " './data/image/test\\\\TEST_00422.jpg',\n",
       " './data/image/test\\\\TEST_00423.jpg',\n",
       " './data/image/test\\\\TEST_00424.jpg',\n",
       " './data/image/test\\\\TEST_00425.jpg',\n",
       " './data/image/test\\\\TEST_00426.jpg',\n",
       " './data/image/test\\\\TEST_00427.jpg',\n",
       " './data/image/test\\\\TEST_00428.jpg',\n",
       " './data/image/test\\\\TEST_00429.jpg',\n",
       " './data/image/test\\\\TEST_00430.jpg',\n",
       " './data/image/test\\\\TEST_00431.jpg',\n",
       " './data/image/test\\\\TEST_00432.jpg',\n",
       " './data/image/test\\\\TEST_00433.jpg',\n",
       " './data/image/test\\\\TEST_00434.jpg',\n",
       " './data/image/test\\\\TEST_00435.jpg',\n",
       " './data/image/test\\\\TEST_00436.jpg',\n",
       " './data/image/test\\\\TEST_00437.jpg',\n",
       " './data/image/test\\\\TEST_00438.jpg',\n",
       " './data/image/test\\\\TEST_00439.jpg',\n",
       " './data/image/test\\\\TEST_00440.jpg',\n",
       " './data/image/test\\\\TEST_00441.jpg',\n",
       " './data/image/test\\\\TEST_00442.jpg',\n",
       " './data/image/test\\\\TEST_00443.jpg',\n",
       " './data/image/test\\\\TEST_00444.jpg',\n",
       " './data/image/test\\\\TEST_00445.jpg',\n",
       " './data/image/test\\\\TEST_00446.jpg',\n",
       " './data/image/test\\\\TEST_00447.jpg',\n",
       " './data/image/test\\\\TEST_00448.jpg',\n",
       " './data/image/test\\\\TEST_00449.jpg',\n",
       " './data/image/test\\\\TEST_00450.jpg',\n",
       " './data/image/test\\\\TEST_00451.jpg',\n",
       " './data/image/test\\\\TEST_00452.jpg',\n",
       " './data/image/test\\\\TEST_00453.jpg',\n",
       " './data/image/test\\\\TEST_00454.jpg',\n",
       " './data/image/test\\\\TEST_00455.jpg',\n",
       " './data/image/test\\\\TEST_00456.jpg',\n",
       " './data/image/test\\\\TEST_00457.jpg',\n",
       " './data/image/test\\\\TEST_00458.jpg',\n",
       " './data/image/test\\\\TEST_00459.jpg',\n",
       " './data/image/test\\\\TEST_00460.jpg',\n",
       " './data/image/test\\\\TEST_00461.jpg',\n",
       " './data/image/test\\\\TEST_00462.jpg',\n",
       " './data/image/test\\\\TEST_00463.jpg',\n",
       " './data/image/test\\\\TEST_00464.jpg',\n",
       " './data/image/test\\\\TEST_00465.jpg',\n",
       " './data/image/test\\\\TEST_00466.jpg',\n",
       " './data/image/test\\\\TEST_00467.jpg',\n",
       " './data/image/test\\\\TEST_00468.jpg',\n",
       " './data/image/test\\\\TEST_00469.jpg',\n",
       " './data/image/test\\\\TEST_00470.jpg',\n",
       " './data/image/test\\\\TEST_00471.jpg',\n",
       " './data/image/test\\\\TEST_00472.jpg',\n",
       " './data/image/test\\\\TEST_00473.jpg',\n",
       " './data/image/test\\\\TEST_00474.jpg',\n",
       " './data/image/test\\\\TEST_00475.jpg',\n",
       " './data/image/test\\\\TEST_00476.jpg',\n",
       " './data/image/test\\\\TEST_00477.jpg',\n",
       " './data/image/test\\\\TEST_00478.jpg',\n",
       " './data/image/test\\\\TEST_00479.jpg',\n",
       " './data/image/test\\\\TEST_00480.jpg',\n",
       " './data/image/test\\\\TEST_00481.jpg',\n",
       " './data/image/test\\\\TEST_00482.jpg',\n",
       " './data/image/test\\\\TEST_00483.jpg',\n",
       " './data/image/test\\\\TEST_00484.jpg',\n",
       " './data/image/test\\\\TEST_00485.jpg',\n",
       " './data/image/test\\\\TEST_00486.jpg',\n",
       " './data/image/test\\\\TEST_00487.jpg',\n",
       " './data/image/test\\\\TEST_00488.jpg',\n",
       " './data/image/test\\\\TEST_00489.jpg',\n",
       " './data/image/test\\\\TEST_00490.jpg',\n",
       " './data/image/test\\\\TEST_00491.jpg',\n",
       " './data/image/test\\\\TEST_00492.jpg',\n",
       " './data/image/test\\\\TEST_00493.jpg',\n",
       " './data/image/test\\\\TEST_00494.jpg',\n",
       " './data/image/test\\\\TEST_00495.jpg',\n",
       " './data/image/test\\\\TEST_00496.jpg',\n",
       " './data/image/test\\\\TEST_00497.jpg',\n",
       " './data/image/test\\\\TEST_00498.jpg',\n",
       " './data/image/test\\\\TEST_00499.jpg',\n",
       " './data/image/test\\\\TEST_00500.jpg',\n",
       " './data/image/test\\\\TEST_00501.jpg',\n",
       " './data/image/test\\\\TEST_00502.jpg',\n",
       " './data/image/test\\\\TEST_00503.jpg',\n",
       " './data/image/test\\\\TEST_00504.jpg',\n",
       " './data/image/test\\\\TEST_00505.jpg',\n",
       " './data/image/test\\\\TEST_00506.jpg',\n",
       " './data/image/test\\\\TEST_00507.jpg',\n",
       " './data/image/test\\\\TEST_00508.jpg',\n",
       " './data/image/test\\\\TEST_00509.jpg',\n",
       " './data/image/test\\\\TEST_00510.jpg',\n",
       " './data/image/test\\\\TEST_00511.jpg',\n",
       " './data/image/test\\\\TEST_00512.jpg',\n",
       " './data/image/test\\\\TEST_00513.jpg',\n",
       " './data/image/test\\\\TEST_00514.jpg',\n",
       " './data/image/test\\\\TEST_00515.jpg',\n",
       " './data/image/test\\\\TEST_00516.jpg',\n",
       " './data/image/test\\\\TEST_00517.jpg',\n",
       " './data/image/test\\\\TEST_00518.jpg',\n",
       " './data/image/test\\\\TEST_00519.jpg',\n",
       " './data/image/test\\\\TEST_00520.jpg',\n",
       " './data/image/test\\\\TEST_00521.jpg',\n",
       " './data/image/test\\\\TEST_00522.jpg',\n",
       " './data/image/test\\\\TEST_00523.jpg',\n",
       " './data/image/test\\\\TEST_00524.jpg',\n",
       " './data/image/test\\\\TEST_00525.jpg',\n",
       " './data/image/test\\\\TEST_00526.jpg',\n",
       " './data/image/test\\\\TEST_00527.jpg',\n",
       " './data/image/test\\\\TEST_00528.jpg',\n",
       " './data/image/test\\\\TEST_00529.jpg',\n",
       " './data/image/test\\\\TEST_00530.jpg',\n",
       " './data/image/test\\\\TEST_00531.jpg',\n",
       " './data/image/test\\\\TEST_00532.jpg',\n",
       " './data/image/test\\\\TEST_00533.jpg',\n",
       " './data/image/test\\\\TEST_00534.jpg',\n",
       " './data/image/test\\\\TEST_00535.jpg',\n",
       " './data/image/test\\\\TEST_00536.jpg',\n",
       " './data/image/test\\\\TEST_00537.jpg',\n",
       " './data/image/test\\\\TEST_00538.jpg',\n",
       " './data/image/test\\\\TEST_00539.jpg',\n",
       " './data/image/test\\\\TEST_00540.jpg',\n",
       " './data/image/test\\\\TEST_00541.jpg',\n",
       " './data/image/test\\\\TEST_00542.jpg',\n",
       " './data/image/test\\\\TEST_00543.jpg',\n",
       " './data/image/test\\\\TEST_00544.jpg',\n",
       " './data/image/test\\\\TEST_00545.jpg',\n",
       " './data/image/test\\\\TEST_00546.jpg',\n",
       " './data/image/test\\\\TEST_00547.jpg',\n",
       " './data/image/test\\\\TEST_00548.jpg',\n",
       " './data/image/test\\\\TEST_00549.jpg',\n",
       " './data/image/test\\\\TEST_00550.jpg',\n",
       " './data/image/test\\\\TEST_00551.jpg',\n",
       " './data/image/test\\\\TEST_00552.jpg',\n",
       " './data/image/test\\\\TEST_00553.jpg',\n",
       " './data/image/test\\\\TEST_00554.jpg',\n",
       " './data/image/test\\\\TEST_00555.jpg',\n",
       " './data/image/test\\\\TEST_00556.jpg',\n",
       " './data/image/test\\\\TEST_00557.jpg',\n",
       " './data/image/test\\\\TEST_00558.jpg',\n",
       " './data/image/test\\\\TEST_00559.jpg',\n",
       " './data/image/test\\\\TEST_00560.jpg',\n",
       " './data/image/test\\\\TEST_00561.jpg',\n",
       " './data/image/test\\\\TEST_00562.jpg',\n",
       " './data/image/test\\\\TEST_00563.jpg',\n",
       " './data/image/test\\\\TEST_00564.jpg',\n",
       " './data/image/test\\\\TEST_00565.jpg',\n",
       " './data/image/test\\\\TEST_00566.jpg',\n",
       " './data/image/test\\\\TEST_00567.jpg',\n",
       " './data/image/test\\\\TEST_00568.jpg',\n",
       " './data/image/test\\\\TEST_00569.jpg',\n",
       " './data/image/test\\\\TEST_00570.jpg',\n",
       " './data/image/test\\\\TEST_00571.jpg',\n",
       " './data/image/test\\\\TEST_00572.jpg',\n",
       " './data/image/test\\\\TEST_00573.jpg',\n",
       " './data/image/test\\\\TEST_00574.jpg',\n",
       " './data/image/test\\\\TEST_00575.jpg',\n",
       " './data/image/test\\\\TEST_00576.jpg',\n",
       " './data/image/test\\\\TEST_00577.jpg',\n",
       " './data/image/test\\\\TEST_00578.jpg',\n",
       " './data/image/test\\\\TEST_00579.jpg',\n",
       " './data/image/test\\\\TEST_00580.jpg',\n",
       " './data/image/test\\\\TEST_00581.jpg',\n",
       " './data/image/test\\\\TEST_00582.jpg',\n",
       " './data/image/test\\\\TEST_00583.jpg',\n",
       " './data/image/test\\\\TEST_00584.jpg',\n",
       " './data/image/test\\\\TEST_00585.jpg',\n",
       " './data/image/test\\\\TEST_00586.jpg',\n",
       " './data/image/test\\\\TEST_00587.jpg',\n",
       " './data/image/test\\\\TEST_00588.jpg',\n",
       " './data/image/test\\\\TEST_00589.jpg',\n",
       " './data/image/test\\\\TEST_00590.jpg',\n",
       " './data/image/test\\\\TEST_00591.jpg',\n",
       " './data/image/test\\\\TEST_00592.jpg',\n",
       " './data/image/test\\\\TEST_00593.jpg',\n",
       " './data/image/test\\\\TEST_00594.jpg',\n",
       " './data/image/test\\\\TEST_00595.jpg',\n",
       " './data/image/test\\\\TEST_00596.jpg',\n",
       " './data/image/test\\\\TEST_00597.jpg',\n",
       " './data/image/test\\\\TEST_00598.jpg',\n",
       " './data/image/test\\\\TEST_00599.jpg',\n",
       " './data/image/test\\\\TEST_00600.jpg',\n",
       " './data/image/test\\\\TEST_00601.jpg',\n",
       " './data/image/test\\\\TEST_00602.jpg',\n",
       " './data/image/test\\\\TEST_00603.jpg',\n",
       " './data/image/test\\\\TEST_00604.jpg',\n",
       " './data/image/test\\\\TEST_00605.jpg',\n",
       " './data/image/test\\\\TEST_00606.jpg',\n",
       " './data/image/test\\\\TEST_00607.jpg',\n",
       " './data/image/test\\\\TEST_00608.jpg',\n",
       " './data/image/test\\\\TEST_00609.jpg',\n",
       " './data/image/test\\\\TEST_00610.jpg',\n",
       " './data/image/test\\\\TEST_00611.jpg',\n",
       " './data/image/test\\\\TEST_00612.jpg',\n",
       " './data/image/test\\\\TEST_00613.jpg',\n",
       " './data/image/test\\\\TEST_00614.jpg',\n",
       " './data/image/test\\\\TEST_00615.jpg',\n",
       " './data/image/test\\\\TEST_00616.jpg',\n",
       " './data/image/test\\\\TEST_00617.jpg',\n",
       " './data/image/test\\\\TEST_00618.jpg',\n",
       " './data/image/test\\\\TEST_00619.jpg',\n",
       " './data/image/test\\\\TEST_00620.jpg',\n",
       " './data/image/test\\\\TEST_00621.jpg',\n",
       " './data/image/test\\\\TEST_00622.jpg',\n",
       " './data/image/test\\\\TEST_00623.jpg',\n",
       " './data/image/test\\\\TEST_00624.jpg',\n",
       " './data/image/test\\\\TEST_00625.jpg',\n",
       " './data/image/test\\\\TEST_00626.jpg',\n",
       " './data/image/test\\\\TEST_00627.jpg',\n",
       " './data/image/test\\\\TEST_00628.jpg',\n",
       " './data/image/test\\\\TEST_00629.jpg',\n",
       " './data/image/test\\\\TEST_00630.jpg',\n",
       " './data/image/test\\\\TEST_00631.jpg',\n",
       " './data/image/test\\\\TEST_00632.jpg',\n",
       " './data/image/test\\\\TEST_00633.jpg',\n",
       " './data/image/test\\\\TEST_00634.jpg',\n",
       " './data/image/test\\\\TEST_00635.jpg',\n",
       " './data/image/test\\\\TEST_00636.jpg',\n",
       " './data/image/test\\\\TEST_00637.jpg',\n",
       " './data/image/test\\\\TEST_00638.jpg',\n",
       " './data/image/test\\\\TEST_00639.jpg',\n",
       " './data/image/test\\\\TEST_00640.jpg',\n",
       " './data/image/test\\\\TEST_00641.jpg',\n",
       " './data/image/test\\\\TEST_00642.jpg',\n",
       " './data/image/test\\\\TEST_00643.jpg',\n",
       " './data/image/test\\\\TEST_00644.jpg',\n",
       " './data/image/test\\\\TEST_00645.jpg',\n",
       " './data/image/test\\\\TEST_00646.jpg',\n",
       " './data/image/test\\\\TEST_00647.jpg',\n",
       " './data/image/test\\\\TEST_00648.jpg',\n",
       " './data/image/test\\\\TEST_00649.jpg',\n",
       " './data/image/test\\\\TEST_00650.jpg',\n",
       " './data/image/test\\\\TEST_00651.jpg',\n",
       " './data/image/test\\\\TEST_00652.jpg',\n",
       " './data/image/test\\\\TEST_00653.jpg',\n",
       " './data/image/test\\\\TEST_00654.jpg',\n",
       " './data/image/test\\\\TEST_00655.jpg',\n",
       " './data/image/test\\\\TEST_00656.jpg',\n",
       " './data/image/test\\\\TEST_00657.jpg',\n",
       " './data/image/test\\\\TEST_00658.jpg',\n",
       " './data/image/test\\\\TEST_00659.jpg',\n",
       " './data/image/test\\\\TEST_00660.jpg',\n",
       " './data/image/test\\\\TEST_00661.jpg',\n",
       " './data/image/test\\\\TEST_00662.jpg',\n",
       " './data/image/test\\\\TEST_00663.jpg',\n",
       " './data/image/test\\\\TEST_00664.jpg',\n",
       " './data/image/test\\\\TEST_00665.jpg',\n",
       " './data/image/test\\\\TEST_00666.jpg',\n",
       " './data/image/test\\\\TEST_00667.jpg',\n",
       " './data/image/test\\\\TEST_00668.jpg',\n",
       " './data/image/test\\\\TEST_00669.jpg',\n",
       " './data/image/test\\\\TEST_00670.jpg',\n",
       " './data/image/test\\\\TEST_00671.jpg',\n",
       " './data/image/test\\\\TEST_00672.jpg',\n",
       " './data/image/test\\\\TEST_00673.jpg',\n",
       " './data/image/test\\\\TEST_00674.jpg',\n",
       " './data/image/test\\\\TEST_00675.jpg',\n",
       " './data/image/test\\\\TEST_00676.jpg',\n",
       " './data/image/test\\\\TEST_00677.jpg',\n",
       " './data/image/test\\\\TEST_00678.jpg',\n",
       " './data/image/test\\\\TEST_00679.jpg',\n",
       " './data/image/test\\\\TEST_00680.jpg',\n",
       " './data/image/test\\\\TEST_00681.jpg',\n",
       " './data/image/test\\\\TEST_00682.jpg',\n",
       " './data/image/test\\\\TEST_00683.jpg',\n",
       " './data/image/test\\\\TEST_00684.jpg',\n",
       " './data/image/test\\\\TEST_00685.jpg',\n",
       " './data/image/test\\\\TEST_00686.jpg',\n",
       " './data/image/test\\\\TEST_00687.jpg',\n",
       " './data/image/test\\\\TEST_00688.jpg',\n",
       " './data/image/test\\\\TEST_00689.jpg',\n",
       " './data/image/test\\\\TEST_00690.jpg',\n",
       " './data/image/test\\\\TEST_00691.jpg',\n",
       " './data/image/test\\\\TEST_00692.jpg',\n",
       " './data/image/test\\\\TEST_00693.jpg',\n",
       " './data/image/test\\\\TEST_00694.jpg',\n",
       " './data/image/test\\\\TEST_00695.jpg',\n",
       " './data/image/test\\\\TEST_00696.jpg',\n",
       " './data/image/test\\\\TEST_00697.jpg',\n",
       " './data/image/test\\\\TEST_00698.jpg',\n",
       " './data/image/test\\\\TEST_00699.jpg',\n",
       " './data/image/test\\\\TEST_00700.jpg',\n",
       " './data/image/test\\\\TEST_00701.jpg',\n",
       " './data/image/test\\\\TEST_00702.jpg',\n",
       " './data/image/test\\\\TEST_00703.jpg',\n",
       " './data/image/test\\\\TEST_00704.jpg',\n",
       " './data/image/test\\\\TEST_00705.jpg',\n",
       " './data/image/test\\\\TEST_00706.jpg',\n",
       " './data/image/test\\\\TEST_00707.jpg',\n",
       " './data/image/test\\\\TEST_00708.jpg',\n",
       " './data/image/test\\\\TEST_00709.jpg',\n",
       " './data/image/test\\\\TEST_00710.jpg',\n",
       " './data/image/test\\\\TEST_00711.jpg',\n",
       " './data/image/test\\\\TEST_00712.jpg',\n",
       " './data/image/test\\\\TEST_00713.jpg',\n",
       " './data/image/test\\\\TEST_00714.jpg',\n",
       " './data/image/test\\\\TEST_00715.jpg',\n",
       " './data/image/test\\\\TEST_00716.jpg',\n",
       " './data/image/test\\\\TEST_00717.jpg',\n",
       " './data/image/test\\\\TEST_00718.jpg',\n",
       " './data/image/test\\\\TEST_00719.jpg',\n",
       " './data/image/test\\\\TEST_00720.jpg',\n",
       " './data/image/test\\\\TEST_00721.jpg',\n",
       " './data/image/test\\\\TEST_00722.jpg',\n",
       " './data/image/test\\\\TEST_00723.jpg',\n",
       " './data/image/test\\\\TEST_00724.jpg',\n",
       " './data/image/test\\\\TEST_00725.jpg',\n",
       " './data/image/test\\\\TEST_00726.jpg',\n",
       " './data/image/test\\\\TEST_00727.jpg',\n",
       " './data/image/test\\\\TEST_00728.jpg',\n",
       " './data/image/test\\\\TEST_00729.jpg',\n",
       " './data/image/test\\\\TEST_00730.jpg',\n",
       " './data/image/test\\\\TEST_00731.jpg',\n",
       " './data/image/test\\\\TEST_00732.jpg',\n",
       " './data/image/test\\\\TEST_00733.jpg',\n",
       " './data/image/test\\\\TEST_00734.jpg',\n",
       " './data/image/test\\\\TEST_00735.jpg',\n",
       " './data/image/test\\\\TEST_00736.jpg',\n",
       " './data/image/test\\\\TEST_00737.jpg',\n",
       " './data/image/test\\\\TEST_00738.jpg',\n",
       " './data/image/test\\\\TEST_00739.jpg',\n",
       " './data/image/test\\\\TEST_00740.jpg',\n",
       " './data/image/test\\\\TEST_00741.jpg',\n",
       " './data/image/test\\\\TEST_00742.jpg',\n",
       " './data/image/test\\\\TEST_00743.jpg',\n",
       " './data/image/test\\\\TEST_00744.jpg',\n",
       " './data/image/test\\\\TEST_00745.jpg',\n",
       " './data/image/test\\\\TEST_00746.jpg',\n",
       " './data/image/test\\\\TEST_00747.jpg',\n",
       " './data/image/test\\\\TEST_00748.jpg',\n",
       " './data/image/test\\\\TEST_00749.jpg',\n",
       " './data/image/test\\\\TEST_00750.jpg',\n",
       " './data/image/test\\\\TEST_00751.jpg',\n",
       " './data/image/test\\\\TEST_00752.jpg',\n",
       " './data/image/test\\\\TEST_00753.jpg',\n",
       " './data/image/test\\\\TEST_00754.jpg',\n",
       " './data/image/test\\\\TEST_00755.jpg',\n",
       " './data/image/test\\\\TEST_00756.jpg',\n",
       " './data/image/test\\\\TEST_00757.jpg',\n",
       " './data/image/test\\\\TEST_00758.jpg',\n",
       " './data/image/test\\\\TEST_00759.jpg',\n",
       " './data/image/test\\\\TEST_00760.jpg',\n",
       " './data/image/test\\\\TEST_00761.jpg',\n",
       " './data/image/test\\\\TEST_00762.jpg',\n",
       " './data/image/test\\\\TEST_00763.jpg',\n",
       " './data/image/test\\\\TEST_00764.jpg',\n",
       " './data/image/test\\\\TEST_00765.jpg',\n",
       " './data/image/test\\\\TEST_00766.jpg',\n",
       " './data/image/test\\\\TEST_00767.jpg',\n",
       " './data/image/test\\\\TEST_00768.jpg',\n",
       " './data/image/test\\\\TEST_00769.jpg',\n",
       " './data/image/test\\\\TEST_00770.jpg',\n",
       " './data/image/test\\\\TEST_00771.jpg',\n",
       " './data/image/test\\\\TEST_00772.jpg',\n",
       " './data/image/test\\\\TEST_00773.jpg',\n",
       " './data/image/test\\\\TEST_00774.jpg',\n",
       " './data/image/test\\\\TEST_00775.jpg',\n",
       " './data/image/test\\\\TEST_00776.jpg',\n",
       " './data/image/test\\\\TEST_00777.jpg',\n",
       " './data/image/test\\\\TEST_00778.jpg',\n",
       " './data/image/test\\\\TEST_00779.jpg',\n",
       " './data/image/test\\\\TEST_00780.jpg',\n",
       " './data/image/test\\\\TEST_00781.jpg',\n",
       " './data/image/test\\\\TEST_00782.jpg',\n",
       " './data/image/test\\\\TEST_00783.jpg',\n",
       " './data/image/test\\\\TEST_00784.jpg',\n",
       " './data/image/test\\\\TEST_00785.jpg',\n",
       " './data/image/test\\\\TEST_00786.jpg',\n",
       " './data/image/test\\\\TEST_00787.jpg',\n",
       " './data/image/test\\\\TEST_00788.jpg',\n",
       " './data/image/test\\\\TEST_00789.jpg',\n",
       " './data/image/test\\\\TEST_00790.jpg',\n",
       " './data/image/test\\\\TEST_00791.jpg',\n",
       " './data/image/test\\\\TEST_00792.jpg',\n",
       " './data/image/test\\\\TEST_00793.jpg',\n",
       " './data/image/test\\\\TEST_00794.jpg',\n",
       " './data/image/test\\\\TEST_00795.jpg',\n",
       " './data/image/test\\\\TEST_00796.jpg',\n",
       " './data/image/test\\\\TEST_00797.jpg',\n",
       " './data/image/test\\\\TEST_00798.jpg',\n",
       " './data/image/test\\\\TEST_00799.jpg',\n",
       " './data/image/test\\\\TEST_00800.jpg',\n",
       " './data/image/test\\\\TEST_00801.jpg',\n",
       " './data/image/test\\\\TEST_00802.jpg',\n",
       " './data/image/test\\\\TEST_00803.jpg',\n",
       " './data/image/test\\\\TEST_00804.jpg',\n",
       " './data/image/test\\\\TEST_00805.jpg',\n",
       " './data/image/test\\\\TEST_00806.jpg',\n",
       " './data/image/test\\\\TEST_00807.jpg',\n",
       " './data/image/test\\\\TEST_00808.jpg',\n",
       " './data/image/test\\\\TEST_00809.jpg',\n",
       " './data/image/test\\\\TEST_00810.jpg',\n",
       " './data/image/test\\\\TEST_00811.jpg',\n",
       " './data/image/test\\\\TEST_00812.jpg',\n",
       " './data/image/test\\\\TEST_00813.jpg',\n",
       " './data/image/test\\\\TEST_00814.jpg',\n",
       " './data/image/test\\\\TEST_00815.jpg',\n",
       " './data/image/test\\\\TEST_00816.jpg',\n",
       " './data/image/test\\\\TEST_00817.jpg',\n",
       " './data/image/test\\\\TEST_00818.jpg',\n",
       " './data/image/test\\\\TEST_00819.jpg',\n",
       " './data/image/test\\\\TEST_00820.jpg',\n",
       " './data/image/test\\\\TEST_00821.jpg',\n",
       " './data/image/test\\\\TEST_00822.jpg',\n",
       " './data/image/test\\\\TEST_00823.jpg',\n",
       " './data/image/test\\\\TEST_00824.jpg',\n",
       " './data/image/test\\\\TEST_00825.jpg',\n",
       " './data/image/test\\\\TEST_00826.jpg',\n",
       " './data/image/test\\\\TEST_00827.jpg',\n",
       " './data/image/test\\\\TEST_00828.jpg',\n",
       " './data/image/test\\\\TEST_00829.jpg',\n",
       " './data/image/test\\\\TEST_00830.jpg',\n",
       " './data/image/test\\\\TEST_00831.jpg',\n",
       " './data/image/test\\\\TEST_00832.jpg',\n",
       " './data/image/test\\\\TEST_00833.jpg',\n",
       " './data/image/test\\\\TEST_00834.jpg',\n",
       " './data/image/test\\\\TEST_00835.jpg',\n",
       " './data/image/test\\\\TEST_00836.jpg',\n",
       " './data/image/test\\\\TEST_00837.jpg',\n",
       " './data/image/test\\\\TEST_00838.jpg',\n",
       " './data/image/test\\\\TEST_00839.jpg',\n",
       " './data/image/test\\\\TEST_00840.jpg',\n",
       " './data/image/test\\\\TEST_00841.jpg',\n",
       " './data/image/test\\\\TEST_00842.jpg',\n",
       " './data/image/test\\\\TEST_00843.jpg',\n",
       " './data/image/test\\\\TEST_00844.jpg',\n",
       " './data/image/test\\\\TEST_00845.jpg',\n",
       " './data/image/test\\\\TEST_00846.jpg',\n",
       " './data/image/test\\\\TEST_00847.jpg',\n",
       " './data/image/test\\\\TEST_00848.jpg',\n",
       " './data/image/test\\\\TEST_00849.jpg',\n",
       " './data/image/test\\\\TEST_00850.jpg',\n",
       " './data/image/test\\\\TEST_00851.jpg',\n",
       " './data/image/test\\\\TEST_00852.jpg',\n",
       " './data/image/test\\\\TEST_00853.jpg',\n",
       " './data/image/test\\\\TEST_00854.jpg',\n",
       " './data/image/test\\\\TEST_00855.jpg',\n",
       " './data/image/test\\\\TEST_00856.jpg',\n",
       " './data/image/test\\\\TEST_00857.jpg',\n",
       " './data/image/test\\\\TEST_00858.jpg',\n",
       " './data/image/test\\\\TEST_00859.jpg',\n",
       " './data/image/test\\\\TEST_00860.jpg',\n",
       " './data/image/test\\\\TEST_00861.jpg',\n",
       " './data/image/test\\\\TEST_00862.jpg',\n",
       " './data/image/test\\\\TEST_00863.jpg',\n",
       " './data/image/test\\\\TEST_00864.jpg',\n",
       " './data/image/test\\\\TEST_00865.jpg',\n",
       " './data/image/test\\\\TEST_00866.jpg',\n",
       " './data/image/test\\\\TEST_00867.jpg',\n",
       " './data/image/test\\\\TEST_00868.jpg',\n",
       " './data/image/test\\\\TEST_00869.jpg',\n",
       " './data/image/test\\\\TEST_00870.jpg',\n",
       " './data/image/test\\\\TEST_00871.jpg',\n",
       " './data/image/test\\\\TEST_00872.jpg',\n",
       " './data/image/test\\\\TEST_00873.jpg',\n",
       " './data/image/test\\\\TEST_00874.jpg',\n",
       " './data/image/test\\\\TEST_00875.jpg',\n",
       " './data/image/test\\\\TEST_00876.jpg',\n",
       " './data/image/test\\\\TEST_00877.jpg',\n",
       " './data/image/test\\\\TEST_00878.jpg',\n",
       " './data/image/test\\\\TEST_00879.jpg',\n",
       " './data/image/test\\\\TEST_00880.jpg',\n",
       " './data/image/test\\\\TEST_00881.jpg',\n",
       " './data/image/test\\\\TEST_00882.jpg',\n",
       " './data/image/test\\\\TEST_00883.jpg',\n",
       " './data/image/test\\\\TEST_00884.jpg',\n",
       " './data/image/test\\\\TEST_00885.jpg',\n",
       " './data/image/test\\\\TEST_00886.jpg',\n",
       " './data/image/test\\\\TEST_00887.jpg',\n",
       " './data/image/test\\\\TEST_00888.jpg',\n",
       " './data/image/test\\\\TEST_00889.jpg',\n",
       " './data/image/test\\\\TEST_00890.jpg',\n",
       " './data/image/test\\\\TEST_00891.jpg',\n",
       " './data/image/test\\\\TEST_00892.jpg',\n",
       " './data/image/test\\\\TEST_00893.jpg',\n",
       " './data/image/test\\\\TEST_00894.jpg',\n",
       " './data/image/test\\\\TEST_00895.jpg',\n",
       " './data/image/test\\\\TEST_00896.jpg',\n",
       " './data/image/test\\\\TEST_00897.jpg',\n",
       " './data/image/test\\\\TEST_00898.jpg',\n",
       " './data/image/test\\\\TEST_00899.jpg',\n",
       " './data/image/test\\\\TEST_00900.jpg',\n",
       " './data/image/test\\\\TEST_00901.jpg',\n",
       " './data/image/test\\\\TEST_00902.jpg',\n",
       " './data/image/test\\\\TEST_00903.jpg',\n",
       " './data/image/test\\\\TEST_00904.jpg',\n",
       " './data/image/test\\\\TEST_00905.jpg',\n",
       " './data/image/test\\\\TEST_00906.jpg',\n",
       " './data/image/test\\\\TEST_00907.jpg',\n",
       " './data/image/test\\\\TEST_00908.jpg',\n",
       " './data/image/test\\\\TEST_00909.jpg',\n",
       " './data/image/test\\\\TEST_00910.jpg',\n",
       " './data/image/test\\\\TEST_00911.jpg',\n",
       " './data/image/test\\\\TEST_00912.jpg',\n",
       " './data/image/test\\\\TEST_00913.jpg',\n",
       " './data/image/test\\\\TEST_00914.jpg',\n",
       " './data/image/test\\\\TEST_00915.jpg',\n",
       " './data/image/test\\\\TEST_00916.jpg',\n",
       " './data/image/test\\\\TEST_00917.jpg',\n",
       " './data/image/test\\\\TEST_00918.jpg',\n",
       " './data/image/test\\\\TEST_00919.jpg',\n",
       " './data/image/test\\\\TEST_00920.jpg',\n",
       " './data/image/test\\\\TEST_00921.jpg',\n",
       " './data/image/test\\\\TEST_00922.jpg',\n",
       " './data/image/test\\\\TEST_00923.jpg',\n",
       " './data/image/test\\\\TEST_00924.jpg',\n",
       " './data/image/test\\\\TEST_00925.jpg',\n",
       " './data/image/test\\\\TEST_00926.jpg',\n",
       " './data/image/test\\\\TEST_00927.jpg',\n",
       " './data/image/test\\\\TEST_00928.jpg',\n",
       " './data/image/test\\\\TEST_00929.jpg',\n",
       " './data/image/test\\\\TEST_00930.jpg',\n",
       " './data/image/test\\\\TEST_00931.jpg',\n",
       " './data/image/test\\\\TEST_00932.jpg',\n",
       " './data/image/test\\\\TEST_00933.jpg',\n",
       " './data/image/test\\\\TEST_00934.jpg',\n",
       " './data/image/test\\\\TEST_00935.jpg',\n",
       " './data/image/test\\\\TEST_00936.jpg',\n",
       " './data/image/test\\\\TEST_00937.jpg',\n",
       " './data/image/test\\\\TEST_00938.jpg',\n",
       " './data/image/test\\\\TEST_00939.jpg',\n",
       " './data/image/test\\\\TEST_00940.jpg',\n",
       " './data/image/test\\\\TEST_00941.jpg',\n",
       " './data/image/test\\\\TEST_00942.jpg',\n",
       " './data/image/test\\\\TEST_00943.jpg',\n",
       " './data/image/test\\\\TEST_00944.jpg',\n",
       " './data/image/test\\\\TEST_00945.jpg',\n",
       " './data/image/test\\\\TEST_00946.jpg',\n",
       " './data/image/test\\\\TEST_00947.jpg',\n",
       " './data/image/test\\\\TEST_00948.jpg',\n",
       " './data/image/test\\\\TEST_00949.jpg',\n",
       " './data/image/test\\\\TEST_00950.jpg',\n",
       " './data/image/test\\\\TEST_00951.jpg',\n",
       " './data/image/test\\\\TEST_00952.jpg',\n",
       " './data/image/test\\\\TEST_00953.jpg',\n",
       " './data/image/test\\\\TEST_00954.jpg',\n",
       " './data/image/test\\\\TEST_00955.jpg',\n",
       " './data/image/test\\\\TEST_00956.jpg',\n",
       " './data/image/test\\\\TEST_00957.jpg',\n",
       " './data/image/test\\\\TEST_00958.jpg',\n",
       " './data/image/test\\\\TEST_00959.jpg',\n",
       " './data/image/test\\\\TEST_00960.jpg',\n",
       " './data/image/test\\\\TEST_00961.jpg',\n",
       " './data/image/test\\\\TEST_00962.jpg',\n",
       " './data/image/test\\\\TEST_00963.jpg',\n",
       " './data/image/test\\\\TEST_00964.jpg',\n",
       " './data/image/test\\\\TEST_00965.jpg',\n",
       " './data/image/test\\\\TEST_00966.jpg',\n",
       " './data/image/test\\\\TEST_00967.jpg',\n",
       " './data/image/test\\\\TEST_00968.jpg',\n",
       " './data/image/test\\\\TEST_00969.jpg',\n",
       " './data/image/test\\\\TEST_00970.jpg',\n",
       " './data/image/test\\\\TEST_00971.jpg',\n",
       " './data/image/test\\\\TEST_00972.jpg',\n",
       " './data/image/test\\\\TEST_00973.jpg',\n",
       " './data/image/test\\\\TEST_00974.jpg',\n",
       " './data/image/test\\\\TEST_00975.jpg',\n",
       " './data/image/test\\\\TEST_00976.jpg',\n",
       " './data/image/test\\\\TEST_00977.jpg',\n",
       " './data/image/test\\\\TEST_00978.jpg',\n",
       " './data/image/test\\\\TEST_00979.jpg',\n",
       " './data/image/test\\\\TEST_00980.jpg',\n",
       " './data/image/test\\\\TEST_00981.jpg',\n",
       " './data/image/test\\\\TEST_00982.jpg',\n",
       " './data/image/test\\\\TEST_00983.jpg',\n",
       " './data/image/test\\\\TEST_00984.jpg',\n",
       " './data/image/test\\\\TEST_00985.jpg',\n",
       " './data/image/test\\\\TEST_00986.jpg',\n",
       " './data/image/test\\\\TEST_00987.jpg',\n",
       " './data/image/test\\\\TEST_00988.jpg',\n",
       " './data/image/test\\\\TEST_00989.jpg',\n",
       " './data/image/test\\\\TEST_00990.jpg',\n",
       " './data/image/test\\\\TEST_00991.jpg',\n",
       " './data/image/test\\\\TEST_00992.jpg',\n",
       " './data/image/test\\\\TEST_00993.jpg',\n",
       " './data/image/test\\\\TEST_00994.jpg',\n",
       " './data/image/test\\\\TEST_00995.jpg',\n",
       " './data/image/test\\\\TEST_00996.jpg',\n",
       " './data/image/test\\\\TEST_00997.jpg',\n",
       " './data/image/test\\\\TEST_00998.jpg',\n",
       " './data/image/test\\\\TEST_00999.jpg',\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "data_path = './data/image/test/*'\n",
    "path = sorted(list(map(lambda x: x.replace(x.split('.')[0], data_path.split('.')[0]), glob(data_path))),\n",
    "                      key=lambda x: int(x.split('\\\\')[-1].split('.')[0].split('_')[-1]))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d18e950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quhb2\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\quhb2\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\quhb2\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\quhb2\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\quhb2\\Anaconda3\\envs\\torch-1.11\\lib\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat3'].values)\n",
    "df['cat3'] = le.transform(df['cat3'].values)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat2'].values)\n",
    "df['cat2'] = le.transform(df['cat2'].values)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat1'].values)\n",
    "df['cat1'] = le.transform(df['cat1'].values)\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "df['kfold']=-1\n",
    "for i in range(5):\n",
    "    df_idx, valid_idx = list(folds.split(df.values, df['cat3']))[i]\n",
    "    valid = df.iloc[valid_idx]\n",
    "    df.loc[df[df.id.isin(valid.id) == True].index.to_list(), 'kfold'] = i\n",
    "df.to_csv('./data/train_folds.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d809682f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>overview</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00005</td>\n",
       "      <td>./image/train/TRAIN_00005.jpg</td>\n",
       "      <td>알파카월드는 우리나라 유일의 알파카 전문 동물원으로 홍천에 있다.  363,600㎡...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00006</td>\n",
       "      <td>./image/train/TRAIN_00006.jpg</td>\n",
       "      <td>진북 편백골 관광농원 캠핑장은 글램핑과 야영장이 함께 운영되는 캠핑장이다. 캠핑장 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00010</td>\n",
       "      <td>./image/train/TRAIN_00010.jpg</td>\n",
       "      <td>외국인 묘지는 인천의 개항과 뿌리를 함께 한다. 이 묘지는 중구 북동동, 율목동, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00018</td>\n",
       "      <td>./image/train/TRAIN_00018.jpg</td>\n",
       "      <td>단일 메뉴를 최고의 맛으로 제공하기 위해 보쌈 한 가지를 판매하고 있다. 맛있고 따...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00022</td>\n",
       "      <td>./image/train/TRAIN_00022.jpg</td>\n",
       "      <td>주양두리돈까스는 암사동 선사고등학교 부근에 있다. 이곳은 26년 전통의 음식점으로 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3393</th>\n",
       "      <td>TRAIN_16964</td>\n",
       "      <td>./image/train/TRAIN_16964.jpg</td>\n",
       "      <td>더리스레스토랑은 대청호 인근에 자리 잡고 있어서 호수의 풍경을 조망하며 식사를 할 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3394</th>\n",
       "      <td>TRAIN_16966</td>\n",
       "      <td>./image/train/TRAIN_16966.jpg</td>\n",
       "      <td>골프장의 위치는 해발 1,100m로 국내 최고 높이에 위치한 골프장이며, 기압이 낮...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>TRAIN_16970</td>\n",
       "      <td>./image/train/TRAIN_16970.jpg</td>\n",
       "      <td>&lt;b&gt;※ 코로나바이러스감염증-19 공지사항&lt;br&gt;※ 내용 : 제한적 개관 (2022...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396</th>\n",
       "      <td>TRAIN_16973</td>\n",
       "      <td>./image/train/TRAIN_16973.jpg</td>\n",
       "      <td>지전이라는 이름은 이 곳이 예전부터 지초(芝草)가 많이 나던 곳이라 하여 붙여졌다고...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>TRAIN_16981</td>\n",
       "      <td>./image/train/TRAIN_16981.jpg</td>\n",
       "      <td>해발 12000m에 자리한 식담겸 카페점문점이다.&lt;br&gt;곤드레밥과 감자전을 판매하고...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3398 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                       img_path  \\\n",
       "0     TRAIN_00005  ./image/train/TRAIN_00005.jpg   \n",
       "1     TRAIN_00006  ./image/train/TRAIN_00006.jpg   \n",
       "2     TRAIN_00010  ./image/train/TRAIN_00010.jpg   \n",
       "3     TRAIN_00018  ./image/train/TRAIN_00018.jpg   \n",
       "4     TRAIN_00022  ./image/train/TRAIN_00022.jpg   \n",
       "...           ...                            ...   \n",
       "3393  TRAIN_16964  ./image/train/TRAIN_16964.jpg   \n",
       "3394  TRAIN_16966  ./image/train/TRAIN_16966.jpg   \n",
       "3395  TRAIN_16970  ./image/train/TRAIN_16970.jpg   \n",
       "3396  TRAIN_16973  ./image/train/TRAIN_16973.jpg   \n",
       "3397  TRAIN_16981  ./image/train/TRAIN_16981.jpg   \n",
       "\n",
       "                                               overview  cat1  cat2  cat3  \\\n",
       "0     알파카월드는 우리나라 유일의 알파카 전문 동물원으로 홍천에 있다.  363,600㎡...     0    11    73   \n",
       "1     진북 편백골 관광농원 캠핑장은 글램핑과 야영장이 함께 운영되는 캠핑장이다. 캠핑장 ...     0    11    73   \n",
       "2     외국인 묘지는 인천의 개항과 뿌리를 함께 한다. 이 묘지는 중구 북동동, 율목동, ...     4    10    85   \n",
       "3     단일 메뉴를 최고의 맛으로 제공하기 위해 보쌈 한 가지를 판매하고 있다. 맛있고 따...     3    12   118   \n",
       "4     주양두리돈까스는 암사동 선사고등학교 부근에 있다. 이곳은 26년 전통의 음식점으로 ...     3    12    91   \n",
       "...                                                 ...   ...   ...   ...   \n",
       "3393  더리스레스토랑은 대청호 인근에 자리 잡고 있어서 호수의 풍경을 조망하며 식사를 할 ...     3    12    57   \n",
       "3394  골프장의 위치는 해발 1,100m로 국내 최고 높이에 위치한 골프장이며, 기압이 낮...     0    11     8   \n",
       "3395  <b>※ 코로나바이러스감염증-19 공지사항<br>※ 내용 : 제한적 개관 (2022...     4     4    97   \n",
       "3396  지전이라는 이름은 이 곳이 예전부터 지초(芝草)가 많이 나던 곳이라 하여 붙여졌다고...     4    10    40   \n",
       "3397  해발 12000m에 자리한 식담겸 카페점문점이다.<br>곤드레밥과 감자전을 판매하고...     3    12   118   \n",
       "\n",
       "      kfold  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "3393      0  \n",
       "3394      0  \n",
       "3395      0  \n",
       "3396      0  \n",
       "3397      0  \n",
       "\n",
       "[3398 rows x 7 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/train_folds.csv')\n",
    "train = df[df[\"kfold\"] != 0].reset_index(drop=True)\n",
    "valid = df[df[\"kfold\"] == 0].reset_index(drop=True)\n",
    "\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "225d97fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>overview</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>./image/train/TRAIN_00004.jpg</td>\n",
       "      <td>※ 영업시간 10:30 ~ 20:30\\n\\n3대에 걸쳐 아귀만을 전문으로 취급하는 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TRAIN_00011</td>\n",
       "      <td>./image/train/TRAIN_00011.jpg</td>\n",
       "      <td>성주 선바위 캠핑장은 경상북도 성주군 금수면 가야산 인근에 자리했다. 후평삼거리에서...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TRAIN_00015</td>\n",
       "      <td>./image/train/TRAIN_00015.jpg</td>\n",
       "      <td>헤이리마을 1 GATE 진입 후 왼쪽 첫 번째 건물에 있는 갤러리이다. 감성 넘치는...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TRAIN_00021</td>\n",
       "      <td>./image/train/TRAIN_00021.jpg</td>\n",
       "      <td>전남 여수시에 위치한 트윈스모텔은 창문마다 작은 테라스가 달려있어 귀여운 느낌의 건...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TRAIN_00023</td>\n",
       "      <td>./image/train/TRAIN_00023.jpg</td>\n",
       "      <td>지산리조트 인근에 위치한 와우스키 렌탈샵은 넓은 주차장과 다양한 장비, 쾌적한 렌탈...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16946</th>\n",
       "      <td>TRAIN_16946</td>\n",
       "      <td>./image/train/TRAIN_16946.jpg</td>\n",
       "      <td>과천의 객사였던 온온사가 축조된 것은 조선 인조 27년(1649년)이었으나 현재의 ...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16968</th>\n",
       "      <td>TRAIN_16968</td>\n",
       "      <td>./image/train/TRAIN_16968.jpg</td>\n",
       "      <td>호구산은 남해 군립공원으로 지정되어 있는데 이것은 그만큼 자연경관이 뛰어나고 보존 ...</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16978</th>\n",
       "      <td>TRAIN_16978</td>\n",
       "      <td>./image/train/TRAIN_16978.jpg</td>\n",
       "      <td>경기 의왕시 오전동에 위치한 도래샘은 촌닭 불고기 쌈밥정식으로 유명한 백운호수 맛집...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16982</th>\n",
       "      <td>TRAIN_16982</td>\n",
       "      <td>./image/train/TRAIN_16982.jpg</td>\n",
       "      <td>설악힐호텔은 동해고속도로 속초톨게이트에서 멀지 않은 관광로 변에 있다. 속초의 대표...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16984</th>\n",
       "      <td>TRAIN_16984</td>\n",
       "      <td>./image/train/TRAIN_16984.jpg</td>\n",
       "      <td>토토큰바위캠핑장은 경기도 가평지역 내에서도 청정지역으로 손꼽히는 지역으로 주변에 화...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3397 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                       img_path  \\\n",
       "4      TRAIN_00004  ./image/train/TRAIN_00004.jpg   \n",
       "11     TRAIN_00011  ./image/train/TRAIN_00011.jpg   \n",
       "15     TRAIN_00015  ./image/train/TRAIN_00015.jpg   \n",
       "21     TRAIN_00021  ./image/train/TRAIN_00021.jpg   \n",
       "23     TRAIN_00023  ./image/train/TRAIN_00023.jpg   \n",
       "...            ...                            ...   \n",
       "16946  TRAIN_16946  ./image/train/TRAIN_16946.jpg   \n",
       "16968  TRAIN_16968  ./image/train/TRAIN_16968.jpg   \n",
       "16978  TRAIN_16978  ./image/train/TRAIN_16978.jpg   \n",
       "16982  TRAIN_16982  ./image/train/TRAIN_16982.jpg   \n",
       "16984  TRAIN_16984  ./image/train/TRAIN_16984.jpg   \n",
       "\n",
       "                                                overview  cat1  cat2  cat3  \\\n",
       "4      ※ 영업시간 10:30 ~ 20:30\\n\\n3대에 걸쳐 아귀만을 전문으로 취급하는 ...     3    12   118   \n",
       "11     성주 선바위 캠핑장은 경상북도 성주군 금수면 가야산 인근에 자리했다. 후평삼거리에서...     0    11    73   \n",
       "15     헤이리마을 1 GATE 진입 후 왼쪽 첫 번째 건물에 있는 갤러리이다. 감성 넘치는...     4     4    97   \n",
       "21     전남 여수시에 위치한 트윈스모텔은 창문마다 작은 테라스가 달려있어 귀여운 느낌의 건...     2     9    31   \n",
       "23     지산리조트 인근에 위치한 와우스키 렌탈샵은 넓은 주차장과 다양한 장비, 쾌적한 렌탈...     0    11    67   \n",
       "...                                                  ...   ...   ...   ...   \n",
       "16946  과천의 객사였던 온온사가 축조된 것은 조선 인조 27년(1649년)이었으나 현재의 ...     4    10    85   \n",
       "16968  호구산은 남해 군립공원으로 지정되어 있는데 이것은 그만큼 자연경관이 뛰어나고 보존 ...     5    13     5   \n",
       "16978  경기 의왕시 오전동에 위치한 도래샘은 촌닭 불고기 쌈밥정식으로 유명한 백운호수 맛집...     3    12   118   \n",
       "16982  설악힐호텔은 동해고속도로 속초톨게이트에서 멀지 않은 관광로 변에 있다. 속초의 대표...     2     9    31   \n",
       "16984  토토큰바위캠핑장은 경기도 가평지역 내에서도 청정지역으로 손꼽히는 지역으로 주변에 화...     0    11    73   \n",
       "\n",
       "       kfold  \n",
       "4          1  \n",
       "11         1  \n",
       "15         1  \n",
       "21         1  \n",
       "23         1  \n",
       "...      ...  \n",
       "16946      1  \n",
       "16968      1  \n",
       "16978      1  \n",
       "16982      1  \n",
       "16984      1  \n",
       "\n",
       "[3397 rows x 7 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['kfold']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6c21468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['cat3'].values)\n",
    "df['cat3'] = le.transform(df['cat3'].values)\n",
    "\n",
    "le.fit(df['cat2'].values)\n",
    "df['cat2'] = le.transform(df['cat2'].values)\n",
    "\n",
    "le.fit(df['cat1'].values)\n",
    "df['cat1'] = le.transform(df['cat1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7639030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/encoded_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f279e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
